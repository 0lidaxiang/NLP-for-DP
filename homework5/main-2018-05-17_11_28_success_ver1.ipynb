{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy.misc import *\n",
    "from skimage.transform import resize as imresize\n",
    "import imageio\n",
    "from glob import glob\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2Pix():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "#         self.img_shape = (self.channels, self.img_rows, self.img_cols)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'train'\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 4\n",
    "        self.df = 4\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        # Input images and their conditioning images\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        valid = self.discriminator([fake_A, img_B])\n",
    "\n",
    "        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        self.combined.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='sigmoid')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A, img_B], validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = Pix2Pix()\n",
    "#     gan.generator.summary()\n",
    "#     gan.discriminator.summary()\n",
    "#     gan.combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def generator_training_Img(real_list_dir,white_list_dir,resize=None,batch_size=32):\n",
    "        batch_real_img=[]\n",
    "        batch_white_img=[]\n",
    "        for _ in range(batch_size):\n",
    "            random_img_index = np.random.randint(0, 100, size=100)[0]\n",
    "            real_img =  imageio.imread(real_list_dir[random_img_index] ,pilmode='L')\n",
    "            white_img =  imageio.imread(white_list_dir[random_img_index] ,pilmode='L')\n",
    "\n",
    "            if resize:\n",
    "                real_img = imresize(real_img,resize, mode='constant')\n",
    "                white_img = imresize(white_img,resize, mode='constant')\n",
    "            batch_real_img.append(real_img)\n",
    "            batch_white_img.append(white_img)\n",
    "        batch_real_img = np.array(batch_real_img)/127.5-1\n",
    "        batch_real_img = np.expand_dims(batch_real_img,axis=1)\n",
    "        batch_white_img = np.array(batch_white_img)/127.5-1\n",
    "        batch_white_img = np.expand_dims(batch_white_img,axis=3)\n",
    "        return batch_real_img,batch_white_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1, 128, 128) (100, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "    train_real_data_dir = r'./datasets/train/Real/*'\n",
    "    train_white_data_dir = r'./datasets/train/White/*'\n",
    "\n",
    "    real_list = glob(train_real_data_dir)\n",
    "    train_real_data_list = []\n",
    "    train_real_data_list.extend(real_list)\n",
    "\n",
    "    white_list = glob(train_white_data_dir)\n",
    "    train_white_data_list = []\n",
    "    train_white_data_list.extend(white_list)\n",
    "    \n",
    "    ori_img,white_img = generator_training_Img(real_list_dir=train_real_data_list,\n",
    "                                               white_list_dir=train_white_data_list,\n",
    "                                               resize=(128,128),\n",
    "                                               batch_size=100)\n",
    "    print(ori_img.shape, white_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/4]  [D loss: 0.114525, acc:  84%] [G loss: 111.233009] time: 0:00:14\n",
      "[Epoch 1/4]  [D loss: 0.043439, acc:  93%] [G loss: 104.579552] time: 0:00:21\n",
      "[Epoch 2/4]  [D loss: 0.025191, acc:  97%] [G loss: 101.581467] time: 0:00:28\n",
      "[Epoch 3/4]  [D loss: 0.012361, acc: 100%] [G loss: 101.189735] time: 0:00:35\n"
     ]
    }
   ],
   "source": [
    "    epochs = 4\n",
    "    batch_size = 1\n",
    "    all_d_loss = np.zeros(epochs)\n",
    "    all_g_loss = np.zeros(epochs)\n",
    "    start_time = datetime.datetime.now()\n",
    "        \n",
    "    # Adversarial loss ground truths\n",
    "    valid = np.ones((batch_size,) + gan.disc_patch)\n",
    "    fake = np.zeros((batch_size,) + gan.disc_patch)\n",
    "        \n",
    "    #  Train Discriminator\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "            narray_imgsB = white_img\n",
    "#             batch_i = 0\n",
    "            for batch_i in range(len(ori_img)):\n",
    "                imgs_A = ori_img[batch_i]\n",
    "                imgs_B = narray_imgsB[batch_i]\n",
    "#                 print(imgs_A.shape)\n",
    "#                 print(imgs_A[0], imgs_B[0])\n",
    "#                 plt.imshow(imgs_B.reshape((128,128)) )\n",
    "#                 plt.imshow(imgs_A.reshape((128,128)) )\n",
    "#                 batch_i += 1\n",
    "                # Condition on B and generate a translated version\n",
    "                imgs_B = imgs_B.reshape((1,128,128,1))\n",
    "                imgs_A = imgs_A.reshape((1,128,128,1))\n",
    "                fake_A = gan.generator.predict(imgs_B)\n",
    "                \n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                # imgs_B is white img\n",
    "                d_loss_real = gan.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = gan.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                # Train the generators\n",
    "                g_loss = gan.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "                    \n",
    "                elapsed_time = str(datetime.datetime.now() - start_time)\n",
    "            all_d_loss[epoch] = d_loss[0]\n",
    "            all_g_loss[epoch] = g_loss[0]\n",
    "            print_out = (epoch, epochs, d_loss[0], 100*d_loss[1], g_loss[0],elapsed_time.split(\".\")[0])\n",
    "            print (\"[Epoch %d/%d]  [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % print_out)\n",
    "            np.savetxt(\"all_d_loss.txt\", all_d_loss, delimiter=\",\")\n",
    "            np.savetxt(\"all_g_loss.txt\", all_g_loss, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     gan.train(epochs= 100, batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "    test_white_list = glob(train_white_data_dir)\n",
    "    test_white_data_list = []\n",
    "    test_white_data_list.extend(test_white_list)\n",
    "    \n",
    "    ori_img,test_white_data_list = generator_training_Img(real_list_dir=train_real_data_list,\n",
    "                                               white_list_dir=test_white_data_list,\n",
    "                                               resize=(128,128),\n",
    "                                               batch_size=10)\n",
    "    imgs_test = test_white_data_list\n",
    "#     imgs_test = imgs_test[:,:,:,:1]\n",
    "    fake_A = gan.generator.predict(test_white_data_list)\n",
    "    gen_imgs = np.concatenate([fake_A])\n",
    "#     gen_imgs = 0.5 * gen_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data generator predict over.\n"
     ]
    }
   ],
   "source": [
    "    ids = 0\n",
    "    for img in gen_imgs:\n",
    "        img = img.reshape((128, 128))\n",
    "        plt.imsave(\"res_images/main_test_res_\" + str(ids) + \".jpg\", img, cmap=\"gray\")\n",
    "        ids += 1                  \n",
    "    plt.close()   \n",
    "    print(\"test_data generator predict over.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw loss \n",
    "all_d_loss_txt = np.loadtxt(\"all_d_loss.txt\")\n",
    "all_g_loss_txt = np.loadtxt(\"all_g_loss.txt\")\n",
    "\n",
    "# print( all_d_loss_txt.shape, all_d_loss_txt.shape[0])\n",
    "# print(all_g_loss_txt, all_g_loss_txt.shape, all_g_loss_txt.shape[0])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "all_d_loss_x = np.linspace(0, 1, all_d_loss_txt.shape[0])\n",
    "all_g_loss_x = np.linspace(0, 1, all_g_loss_txt.shape[0])\n",
    "\n",
    "plt.plot(all_g_loss_x, all_g_loss_txt, '-r');  # dotted red, g_loss\n",
    "plt.plot(all_d_loss_x , all_d_loss_txt , '-g');  # dotted green, d_loss\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     gan.sample_images(20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss = np.array([1,2,3])\n",
    "# ss[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
