{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "# import keras\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN, GRU,Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from datetime import datetime\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts data\n",
    "category2idx = {'Japan_Travel': 0, 'KR_ENTERTAIN': 1, 'Makeup': 2, 'Tech_Job':  3, 'WomenTalk': 4,\n",
    "                  'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "\n",
    "train_df_sample = pd.read_pickle('train.pkl').sample(frac=1, random_state=123)\n",
    "# train_df_sample = pd.concat([train_pickle_df.text]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embedding vector\n",
    "answer = word2vec.Word2Vec.load(\"word2vec_20180424.model\")\n",
    "# print(type(answer))\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "wvv_keys = wvv.keys()\n",
    "wvv_keys_list = list(wvv_keys)\n",
    "# print(wvv_keys_list[:10]) #['櫻花林', '好比', '考科', '床上', '一點現', '記住', '寶寶的', '柔嫩', '不規則', '朴智妍']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df_sample.values\n",
    "label_list = to_categorical(train_df_sample.category)\n",
    "\n",
    "test_pickle_df = pd.read_pickle('test.pkl')\n",
    "test_texts = test_pickle_df[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word_index[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "#         new_doc_arr = np.array(new_doc).reshape(1, max_doc_word_length)\n",
    "        new_doc_arr = np.array(new_doc)\n",
    "        new_corpus.append( new_doc_arr)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 ['挑色', '挑', '尺寸', '挑', '款式', '挑', '尺寸', '人', '資料', '簡述', '胸無肉', '最近', '黛妮', '蠶絲', '衣尺', '碼為', '32D', '上圍', '84', '下圍', '6566', '這幾件', '內衣', '尺寸', '謎樣', '實在', '不想', '買', '再', '轉讓', '想', '說', '最近', '收到', '這件', '內衣', '請', '幫幫', '挑選', '挑選', '需求', '能夠', '集中', '不調', '皮亂', '跑', 'spanclasshl', '選手', '號', 'span', '物品', '連結', 'ahrefhttpgooglMHr6zrelnofollowtargetblankhttpgooglMHr6za', '集中', '款猶豫', '不決', '32C', '32Dspanclasshl', '選手', '號', 'span', '物品', '連結', 'ahrefhttpgooglPbBOvrelnofollowtargetblankhttpgooglPbBOva', '深', 'V', '款猶豫', '不決', '32C', '32B']\n"
     ]
    }
   ],
   "source": [
    "train_texts_list = []\n",
    "for text in train_texts:\n",
    "    train_texts_list.append(text[0])\n",
    "\n",
    "# train_texts_index = train_texts_list\n",
    "\n",
    "print(len(train_texts_list), train_texts_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_texts_list1 = list(train_texts_list)\n",
    "# print(type(train_texts_list), len(train_texts_list), type(train_texts_list[0]))\n",
    "# print(train_texts_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 232236 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts_list)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts_list)\n",
    "# max_doc_word_length = max(len(l) for l in train_texts)\n",
    "max_doc_word_length = 200\n",
    "sequences1 = pad_sequences(sequences, maxlen=max_doc_word_length, padding='post')\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "\n",
    "# data = pad_sequences(sequences)\n",
    "# print(\"Shape of data tensor:\" , data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "# create a weight matrix for words in training docs\n",
    "answer_vector_size = answer.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, answer_vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in wvv_keys_list:\n",
    "        embedding_vector = answer[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i+1] = embedding_vector\n",
    "#     else:\n",
    "#         embedding_matrix[i] = np.zeros((1, answer_vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 250)         58059250  \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               1700      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 58,084,876\n",
      "Trainable params: 25,626\n",
      "Non-trainable params: 58,059,250\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim= embedding_matrix.shape[0],\n",
    "                            output_dim= embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix], \n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(16))    \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=0, mode='auto')\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = text_to_index(train_df_sample.text)\n",
    "X_train = pad_sequences(X_train_texts, maxlen= max_doc_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 <class 'numpy.ndarray'>\n",
      "200 <class 'numpy.ndarray'>\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# X_train_texts_ = list(X_train_texts)\n",
    "print(len(X_train), type(X_train))\n",
    "print(len(X_train[0]), type(X_train[0]))\n",
    "print(X_train[0].shape)\n",
    "print(X_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/100\n",
      "8100/8100 [==============================] - 18s 2ms/step - loss: 2.3020 - acc: 0.1106 - val_loss: 2.2934 - val_acc: 0.1144\n",
      "Epoch 2/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2863 - acc: 0.1369 - val_loss: 2.2810 - val_acc: 0.1244\n",
      "Epoch 3/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.2722 - acc: 0.1522 - val_loss: 2.2680 - val_acc: 0.1356\n",
      "Epoch 4/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.2575 - acc: 0.1542 - val_loss: 2.2541 - val_acc: 0.1522\n",
      "Epoch 5/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.2416 - acc: 0.1691 - val_loss: 2.2401 - val_acc: 0.1600\n",
      "Epoch 6/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2259 - acc: 0.1801 - val_loss: 2.2263 - val_acc: 0.1656\n",
      "Epoch 7/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2111 - acc: 0.1914 - val_loss: 2.2132 - val_acc: 0.1811\n",
      "Epoch 8/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 2.1967 - acc: 0.2038 - val_loss: 2.2002 - val_acc: 0.1989\n",
      "Epoch 9/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1832 - acc: 0.2159 - val_loss: 2.1874 - val_acc: 0.2078\n",
      "Epoch 10/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1694 - acc: 0.2223 - val_loss: 2.1748 - val_acc: 0.2067\n",
      "Epoch 11/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1561 - acc: 0.2298 - val_loss: 2.1631 - val_acc: 0.2122\n",
      "Epoch 12/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1421 - acc: 0.2377 - val_loss: 2.1534 - val_acc: 0.2222\n",
      "Epoch 13/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1294 - acc: 0.2419 - val_loss: 2.1455 - val_acc: 0.2189\n",
      "Epoch 14/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1160 - acc: 0.2437 - val_loss: 2.1365 - val_acc: 0.2289\n",
      "Epoch 15/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1026 - acc: 0.2484 - val_loss: 2.1275 - val_acc: 0.2367\n",
      "Epoch 16/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.0891 - acc: 0.2572 - val_loss: 2.1185 - val_acc: 0.2478\n",
      "Epoch 17/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.0754 - acc: 0.2662 - val_loss: 2.1091 - val_acc: 0.2533\n",
      "Epoch 18/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.0612 - acc: 0.2706 - val_loss: 2.0991 - val_acc: 0.2678\n",
      "Epoch 19/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.0465 - acc: 0.2786 - val_loss: 2.0894 - val_acc: 0.2589\n",
      "Epoch 20/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.0321 - acc: 0.2837 - val_loss: 2.0787 - val_acc: 0.2689\n",
      "Epoch 21/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.0163 - acc: 0.2895 - val_loss: 2.0678 - val_acc: 0.2711\n",
      "Epoch 22/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.0003 - acc: 0.2965 - val_loss: 2.0579 - val_acc: 0.2778\n",
      "Epoch 23/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.9838 - acc: 0.3023 - val_loss: 2.0473 - val_acc: 0.2889\n",
      "Epoch 24/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.9667 - acc: 0.3086 - val_loss: 2.0359 - val_acc: 0.2900\n",
      "Epoch 25/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.9486 - acc: 0.3173 - val_loss: 2.0256 - val_acc: 0.3011\n",
      "Epoch 26/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.9300 - acc: 0.3221 - val_loss: 2.0129 - val_acc: 0.2889\n",
      "Epoch 27/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.9105 - acc: 0.3281 - val_loss: 2.0010 - val_acc: 0.3000\n",
      "Epoch 28/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.8910 - acc: 0.3372 - val_loss: 1.9869 - val_acc: 0.3000\n",
      "Epoch 29/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.8714 - acc: 0.3449 - val_loss: 1.9720 - val_acc: 0.3033\n",
      "Epoch 30/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.8515 - acc: 0.3538 - val_loss: 1.9565 - val_acc: 0.3133\n",
      "Epoch 31/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.8288 - acc: 0.3632 - val_loss: 1.9394 - val_acc: 0.3167\n",
      "Epoch 32/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.8059 - acc: 0.3736 - val_loss: 1.9218 - val_acc: 0.3256\n",
      "Epoch 33/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.7826 - acc: 0.3801 - val_loss: 1.8991 - val_acc: 0.3344\n",
      "Epoch 34/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.7580 - acc: 0.3930 - val_loss: 1.8737 - val_acc: 0.3467\n",
      "Epoch 35/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.7318 - acc: 0.3993 - val_loss: 1.8463 - val_acc: 0.3578\n",
      "Epoch 36/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.7045 - acc: 0.4070 - val_loss: 1.8146 - val_acc: 0.3600\n",
      "Epoch 37/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.6739 - acc: 0.4164 - val_loss: 1.7882 - val_acc: 0.3711\n",
      "Epoch 38/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.6432 - acc: 0.4247 - val_loss: 1.7469 - val_acc: 0.3722\n",
      "Epoch 39/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.6119 - acc: 0.4364 - val_loss: 1.7150 - val_acc: 0.3889\n",
      "Epoch 40/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.5811 - acc: 0.4459 - val_loss: 1.6906 - val_acc: 0.3956\n",
      "Epoch 41/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.5476 - acc: 0.4581 - val_loss: 1.6774 - val_acc: 0.3922\n",
      "Epoch 42/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.5210 - acc: 0.4674 - val_loss: 1.6292 - val_acc: 0.4233\n",
      "Epoch 43/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.4905 - acc: 0.4775 - val_loss: 1.6087 - val_acc: 0.4300\n",
      "Epoch 44/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.4668 - acc: 0.4865 - val_loss: 1.5896 - val_acc: 0.4400\n",
      "Epoch 45/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.4432 - acc: 0.4931 - val_loss: 1.5821 - val_acc: 0.4411\n",
      "Epoch 46/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.4216 - acc: 0.5006 - val_loss: 1.5631 - val_acc: 0.4389\n",
      "Epoch 47/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.3998 - acc: 0.5075 - val_loss: 1.5484 - val_acc: 0.4489\n",
      "Epoch 48/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.3784 - acc: 0.5153 - val_loss: 1.5220 - val_acc: 0.4656\n",
      "Epoch 49/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.3566 - acc: 0.5221 - val_loss: 1.4984 - val_acc: 0.4756\n",
      "Epoch 50/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.3371 - acc: 0.5257 - val_loss: 1.4810 - val_acc: 0.4867\n",
      "Epoch 51/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.3172 - acc: 0.5337 - val_loss: 1.4705 - val_acc: 0.4800\n",
      "Epoch 52/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2992 - acc: 0.5436 - val_loss: 1.4643 - val_acc: 0.4911\n",
      "Epoch 53/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2855 - acc: 0.5457 - val_loss: 1.4641 - val_acc: 0.4889\n",
      "Epoch 54/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2714 - acc: 0.5519 - val_loss: 1.4338 - val_acc: 0.4967\n",
      "Epoch 55/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2508 - acc: 0.5609 - val_loss: 1.4093 - val_acc: 0.5056\n",
      "Epoch 56/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2323 - acc: 0.5711 - val_loss: 1.3867 - val_acc: 0.5178\n",
      "Epoch 57/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2156 - acc: 0.5772 - val_loss: 1.3657 - val_acc: 0.5278\n",
      "Epoch 58/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1975 - acc: 0.5844 - val_loss: 1.3504 - val_acc: 0.5333\n",
      "Epoch 59/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1830 - acc: 0.5909 - val_loss: 1.3457 - val_acc: 0.5289\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1709 - acc: 0.5988 - val_loss: 1.3478 - val_acc: 0.5333\n",
      "Epoch 61/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1571 - acc: 0.6006 - val_loss: 1.3288 - val_acc: 0.5500\n",
      "Epoch 62/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1412 - acc: 0.6052 - val_loss: 1.3052 - val_acc: 0.5578\n",
      "Epoch 63/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1262 - acc: 0.6084 - val_loss: 1.2933 - val_acc: 0.5611\n",
      "Epoch 64/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1141 - acc: 0.6128 - val_loss: 1.2862 - val_acc: 0.5689\n",
      "Epoch 65/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1013 - acc: 0.6177 - val_loss: 1.2819 - val_acc: 0.5733\n",
      "Epoch 66/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0875 - acc: 0.6230 - val_loss: 1.2674 - val_acc: 0.5667\n",
      "Epoch 67/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0745 - acc: 0.6314 - val_loss: 1.2622 - val_acc: 0.5656\n",
      "Epoch 68/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0632 - acc: 0.6346 - val_loss: 1.2578 - val_acc: 0.5678\n",
      "Epoch 69/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0527 - acc: 0.6412 - val_loss: 1.2370 - val_acc: 0.5756\n",
      "Epoch 70/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0415 - acc: 0.6430 - val_loss: 1.2404 - val_acc: 0.5700\n",
      "Epoch 71/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0316 - acc: 0.6452 - val_loss: 1.2262 - val_acc: 0.5778\n",
      "Epoch 72/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0211 - acc: 0.6498 - val_loss: 1.2158 - val_acc: 0.5867\n",
      "Epoch 73/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0116 - acc: 0.6531 - val_loss: 1.2057 - val_acc: 0.5811\n",
      "Epoch 74/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0014 - acc: 0.6569 - val_loss: 1.1997 - val_acc: 0.5811\n",
      "Epoch 75/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.9908 - acc: 0.6610 - val_loss: 1.1907 - val_acc: 0.5856\n",
      "Epoch 76/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.9821 - acc: 0.6644 - val_loss: 1.1881 - val_acc: 0.5856\n",
      "Epoch 77/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.9763 - acc: 0.6659 - val_loss: 1.1805 - val_acc: 0.5911\n",
      "Epoch 78/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.9686 - acc: 0.6670 - val_loss: 1.1688 - val_acc: 0.5944\n",
      "Epoch 79/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.9560 - acc: 0.6737 - val_loss: 1.1719 - val_acc: 0.5844\n",
      "Epoch 80/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.9459 - acc: 0.6777 - val_loss: 1.1574 - val_acc: 0.5900\n",
      "Epoch 81/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.9363 - acc: 0.6805 - val_loss: 1.1515 - val_acc: 0.5978\n",
      "Epoch 82/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.9291 - acc: 0.6819 - val_loss: 1.1436 - val_acc: 0.6044\n",
      "Epoch 83/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.9215 - acc: 0.6865 - val_loss: 1.1488 - val_acc: 0.6000\n",
      "Epoch 84/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.9158 - acc: 0.6879 - val_loss: 1.1343 - val_acc: 0.6056\n",
      "Epoch 85/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 0.9077 - acc: 0.6888 - val_loss: 1.1335 - val_acc: 0.6033\n",
      "Epoch 86/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.8958 - acc: 0.6951 - val_loss: 1.1254 - val_acc: 0.6044\n",
      "Epoch 87/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8868 - acc: 0.6979 - val_loss: 1.1196 - val_acc: 0.6178\n",
      "Epoch 88/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 0.8796 - acc: 0.6990 - val_loss: 1.1124 - val_acc: 0.6133\n",
      "Epoch 89/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 0.8733 - acc: 0.7028 - val_loss: 1.1047 - val_acc: 0.6167\n",
      "Epoch 90/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8710 - acc: 0.7020 - val_loss: 1.1032 - val_acc: 0.6100\n",
      "Epoch 91/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8636 - acc: 0.7036 - val_loss: 1.1143 - val_acc: 0.6156\n",
      "Epoch 92/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8602 - acc: 0.7049 - val_loss: 1.0916 - val_acc: 0.6200\n",
      "Epoch 93/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8589 - acc: 0.7080 - val_loss: 1.0897 - val_acc: 0.6344\n",
      "Epoch 94/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8497 - acc: 0.7095 - val_loss: 1.0861 - val_acc: 0.6300\n",
      "Epoch 95/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.8423 - acc: 0.7122 - val_loss: 1.0752 - val_acc: 0.6289\n",
      "Epoch 96/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8341 - acc: 0.7151 - val_loss: 1.1191 - val_acc: 0.6289\n",
      "Epoch 97/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8498 - acc: 0.7049 - val_loss: 1.0640 - val_acc: 0.6333\n",
      "Epoch 98/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.8221 - acc: 0.7219 - val_loss: 1.0730 - val_acc: 0.6289\n",
      "Epoch 99/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.8182 - acc: 0.7215 - val_loss: 1.0570 - val_acc: 0.6378\n",
      "Epoch 100/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 0.8079 - acc: 0.7270 - val_loss: 1.0645 - val_acc: 0.6356\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = label_list, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size= 3000,\n",
    "                    epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func_name = 'categorical_crossentropy'\n",
    "# for value in history.history[\"categorical_crossentropy\"]:\n",
    "#     print(history)\n",
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step\n",
      "<class 'list'> [2.1207345390319823, 0.42]\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss_accuracy = model.evaluate(sequences1[0:100], label_list[0:100], verbose=1)\n",
    "print(type(loss_accuracy), loss_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tokenizer = Tokenizer()\n",
    "# test_tokenizer.fit_on_texts(test_texts)\n",
    "# test_sequences = test_tokenizer.texts_to_sequences(test_texts)\n",
    "# test_sequences1 = pad_sequences(test_sequences, maxlen=max_doc_word_length, padding='post')\n",
    "\n",
    "Y_sequences1 = text_to_index(test_pickle_df.text)\n",
    "Y_sequences11 = pad_sequences(Y_sequences1, maxlen=max_doc_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "[4, 9, 4, 5, 8, 8, 8, 1, 0, 0, 5, 6, 6, 8, 7, 8, 9, 3, 3, 8, 5, 3, 9, 2, 7, 6, 8, 6, 8, 4, 0, 0, 4, 9, 3, 7, 5, 6, 5, 7, 6, 9, 3, 5, 8, 9, 6, 0, 8, 2, 5, 9, 7, 3, 6, 7, 5, 9, 3, 5, 5, 5, 0, 5, 1, 5, 9, 9, 0, 0, 3, 5, 0, 7, 8, 7, 1, 4, 4, 2, 6, 4, 1, 4, 4, 2, 2, 5, 2, 0, 4, 3, 5, 1, 9, 3, 6, 2, 4, 4, 7, 9, 2, 5, 3, 7, 3, 6, 5, 3, 1, 8, 8, 1, 4, 2, 9, 5, 7, 5, 8, 1, 5, 2, 8, 3, 1, 7, 5, 1, 4, 2, 4, 7, 8, 8, 5, 5, 1, 0, 5, 7, 4, 3, 2, 2, 2, 3, 0, 0, 7, 3, 9, 2, 4, 8, 0, 1, 2, 1, 2, 2, 8, 1, 2, 3, 8, 4, 6, 7, 7, 4, 1, 3, 3, 4, 2, 7, 9, 9, 1, 3, 8, 2, 2, 7, 4, 7, 0, 0, 3, 7, 1, 5, 5, 9, 8, 3, 4, 5, 7, 7, 2, 8, 1, 7, 9, 3, 3, 9, 8, 2, 4, 6, 3, 4, 6, 7, 2, 6, 0, 6, 2, 8, 9, 3, 8, 2, 9, 9, 1, 3, 5, 3, 4, 6, 9, 4, 3, 2, 9, 9, 6, 2, 6, 1, 6, 7, 8, 3, 5, 7, 0, 6, 1, 5, 4, 5, 4, 3, 7, 8, 1, 1, 2, 2, 2, 0, 7, 3, 7, 9, 6, 8, 4, 1, 7, 9, 9, 0, 1, 5, 8, 7, 3, 5, 5, 1, 2, 6, 1, 0, 9, 7, 9, 3, 6, 1, 5, 1, 0, 7, 0, 7, 5, 4, 7, 5, 7, 0, 6, 8, 6, 9, 4, 0, 3, 2, 1, 1, 4, 8, 0, 6, 0, 4, 5, 9, 1, 3, 2, 6, 7, 6, 0, 0, 4, 7, 3, 1, 9, 7, 2, 5, 7, 1, 0, 0, 1, 8, 0, 0, 9, 0, 5, 9, 5, 3, 3, 7, 2, 0, 8, 3, 3, 8, 8, 7, 9, 0, 8, 2, 0, 1, 6, 7, 2, 1, 2, 8, 1, 6, 8, 6, 8, 6, 5, 6, 8, 0, 6, 5, 5, 2, 5, 5, 4, 2, 4, 3, 6, 8, 8, 9, 3, 4, 8, 4, 6, 9, 7, 8, 4, 7, 5, 6, 0, 8, 8, 0, 8, 9, 6, 3, 8, 2, 2, 8, 1, 4, 5, 9, 3, 5, 4, 6, 5, 7, 4, 5, 7, 7, 2, 0, 5, 1, 8, 4, 2, 9, 6, 8, 4, 0, 8, 1, 2, 3, 2, 9, 7, 6, 1, 1, 0, 5, 3, 2, 3, 1, 9, 3, 4, 1, 3, 4, 7, 4, 0, 8, 0, 8, 7, 7, 8, 9, 9, 5, 1, 1, 3, 7, 7, 0, 2, 6, 8, 8, 5, 8, 0, 8, 2, 8, 2, 7, 5, 8, 1, 9, 5, 4, 4, 7, 6, 5, 0, 5, 6, 9, 4, 5, 9, 6, 3, 7, 7, 8, 5, 4, 2, 4, 4, 5, 4, 6, 1, 6, 3, 1, 1, 6, 0, 8, 5, 7, 0, 3, 7, 0, 9, 2, 3, 4, 8, 8, 0, 0, 9, 2, 8, 8, 4, 8, 4, 1, 2, 2, 8, 6, 4, 3, 2, 5, 7, 5, 2, 1, 3, 8, 8, 9, 2, 2, 8, 6, 2, 1, 7, 4, 0, 9, 2, 7, 7, 1, 8, 9, 2, 9, 4, 6, 5, 1, 1, 7, 5, 9, 4, 7, 0, 4, 1, 8, 5, 0, 3, 3, 8, 1, 2, 8, 2, 7, 8, 4, 7, 1, 9, 3, 5, 0, 8, 4, 0, 5, 1, 5, 5, 7, 7, 4, 6, 8, 7, 7, 2, 6, 4, 3, 7, 8, 3, 8, 5, 7, 4, 7, 8, 0, 0, 7, 0, 9, 5, 9, 6, 7, 3, 3, 8, 9, 2, 5, 3, 0, 7, 3, 4, 8, 8, 7, 3, 8, 7, 0, 7, 0, 3, 4, 5, 2, 4, 3, 2, 8, 6, 4, 8, 7, 5, 1, 2, 1, 2, 6, 4, 1, 4, 8, 8, 3, 0, 5, 4, 2, 1, 2, 8, 5, 4, 2, 8, 1, 8, 6, 8, 0, 9, 1, 7, 6, 2, 3, 9, 4, 9, 6, 4, 2, 3, 0, 8, 9, 6, 9, 5, 8, 5, 2, 6, 6, 7, 3, 2, 3, 6, 0, 4, 6, 0, 2, 7, 2, 2, 5, 7, 1, 4, 4, 1, 0, 0, 5, 7, 0, 0, 1, 9, 7, 0, 0, 2, 9, 1, 6, 8, 8, 6, 1, 0, 6, 5, 2, 3, 2, 4, 6, 3, 3, 3, 2, 1, 4, 6, 5, 4, 7, 6, 9, 0, 7, 9, 5, 3, 8, 9, 5, 7, 9, 6, 2, 7, 9, 8, 1, 4, 9, 3, 8, 5, 8, 2, 3, 4, 2, 3, 2, 2, 5, 4, 2, 5, 1, 0, 4, 5, 4, 5, 5, 4, 4, 0, 6, 6, 6, 8, 1, 4, 2, 2, 8, 0, 8, 1, 3, 5, 5, 3, 2, 0, 6, 8, 7, 8, 6, 1, 7, 7, 0, 5, 0, 3, 2, 6, 4, 4, 8, 6, 6, 7, 8, 4, 6, 9, 8, 5, 1, 9, 3, 4, 6, 2, 1, 8, 1, 5, 5, 7, 3, 0, 9, 5, 8, 7, 8, 8, 8, 8, 1, 6, 6, 9, 7, 9, 1, 5, 8, 3, 1, 5, 8, 4, 5, 9, 9, 1, 3, 3, 8, 9, 5, 5, 8, 2, 6, 8, 4, 7, 9, 6, 7, 0, 9, 6, 1, 7, 8, 4, 0, 4, 9, 1, 6, 6, 2, 6, 2, 6, 8, 7, 6, 1, 8, 3, 5, 6, 5, 1, 3, 3, 1, 4, 0, 9, 3, 5, 8, 1, 5, 6, 2, 7, 1, 5, 0, 9, 5, 7, 5]\n"
     ]
    }
   ],
   "source": [
    "predict_res = model.predict(Y_sequences11, verbose=1)\n",
    "# print(len(predict_res), predict_res)\n",
    "\n",
    "final_res = []\n",
    "for pre_res in predict_res:\n",
    "    final_res.append(np.argmax(pre_res))\n",
    "print(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "print(len(final_res))\n",
    "result_txt = \"result001\" + \".txt\"\n",
    "ids = 0\n",
    "with open(result_txt, 'w') as out:\n",
    "    out.write(\"id,category\" + '\\n')\n",
    "    for value in final_res:\n",
    "        out.write(str(ids) + \",\" + str(value) + '\\n')\n",
    "        ids += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summ = 0\n",
    "# for s in test_texts[100:110]:\n",
    "# #     summ += len(s)\n",
    "#     print(s)\n",
    "# # print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
