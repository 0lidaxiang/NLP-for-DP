{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "# import keras\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN, GRU,Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from datetime import datetime\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts data\n",
    "category2idx = {'Japan_Travel': 0, 'KR_ENTERTAIN': 1, 'Makeup': 2, 'Tech_Job':  3, 'WomenTalk': 4,\n",
    "                  'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "\n",
    "train_df_sample = pd.read_pickle('train.pkl').sample(frac=1, random_state=123)\n",
    "# train_df_sample = pd.concat([train_pickle_df.text]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embedding vector\n",
    "answer = word2vec.Word2Vec.load(\"word2vec_20180425.model\")\n",
    "# print(type(answer))\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "wvv_keys = wvv.keys()\n",
    "wvv_keys_list = list(wvv_keys)\n",
    "# print(wvv_keys_list[:10]) #['櫻花林', '好比', '考科', '床上', '一點現', '記住', '寶寶的', '柔嫩', '不規則', '朴智妍']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df_sample.values\n",
    "label_list = to_categorical(train_df_sample.category)\n",
    "\n",
    "test_pickle_df = pd.read_pickle('test.pkl')\n",
    "test_texts = test_pickle_df[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_list = []\n",
    "for text in train_texts:\n",
    "    train_texts_list.append(text[0])\n",
    "\n",
    "# train_texts_index = train_texts_list\n",
    "\n",
    "# print(len(train_texts_list), train_texts_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = []\n",
    "for text in train_texts_list:\n",
    "    texts_list.append(text)\n",
    "    \n",
    "for text in test_texts:\n",
    "    texts_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 218242 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_list)\n",
    "sequences = tokenizer.texts_to_sequences(texts_list)\n",
    "# max_doc_word_length = max(len(l) for l in train_texts)\n",
    "max_doc_word_length = 200\n",
    "sequences1 = pad_sequences(sequences, maxlen=max_doc_word_length, padding='pre')\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "\n",
    "del texts_list,train_texts_list,test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'> 218242\n",
      "<class 'list'> 218242\n",
      "218242\n",
      "34453\n",
      "41796\n"
     ]
    }
   ],
   "source": [
    "word_counts = tokenizer.word_counts\n",
    "print(type(word_counts), len(word_counts))\n",
    "new_word_counts = list(word_counts.items())\n",
    "print(type(new_word_counts), len(new_word_counts))\n",
    "new_word_counts = sorted(new_word_counts, key=lambda s: s[1], reverse=True)\n",
    "ii = 0\n",
    "print(len(word_index))\n",
    "# new_word_counts[20:41142]\n",
    "for word in new_word_counts:\n",
    "    if word[1] < 4:\n",
    "        del word_index[word[0]]\n",
    "    if word[1] > 4000:\n",
    "#         print(word[0], word[1])\n",
    "        del word_index[word[0]]\n",
    "print(len(wvv_keys_list))\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others_words = []\n",
    "# for k,w in word_index.items():\n",
    "#     if w >41797:\n",
    "# #         del word_index[k]\n",
    "#         others_words.append(k)\n",
    "#         print(k,w)\n",
    "# for w in others_words:\n",
    "#     del word_index[w]\n",
    "# print(\"-------------\") \n",
    "\n",
    "word_index_list= []\n",
    "for key, value in word_index.items():\n",
    "    temp = [key,value]\n",
    "    word_index_list.append(key)\n",
    "del word_index\n",
    "new_word_index = {word_index_list[i]: i for i in range(0, len(word_index_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41796\n",
      "['張', '戴隱形', '寫實', '聊得', '北美票房']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(new_word_index))\n",
    "print(word_index_list[:5])\n",
    "print(new_word_index['戴隱形'])\n",
    "# for k,w in word_index.items():\n",
    "# #     if w >41797:\n",
    "# # #         del word_index[k]\n",
    "# #         others_words.append(k)\n",
    "#         print(k,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41797\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(new_word_index) + 1\n",
    "# create a weight matrix for words in training docs\n",
    "answer_vector_size = answer.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, answer_vector_size))\n",
    "embed_i = 1\n",
    "for word in new_word_index:\n",
    "    if word in wvv_keys_list:\n",
    "        embedding_vector = answer[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[embed_i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[embed_i] = np.zeros((1, answer_vector_size))\n",
    "    embed_i += 1\n",
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "del answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 250)         10449250  \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               1700      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 10,474,876\n",
      "Trainable params: 25,626\n",
      "Non-trainable params: 10,449,250\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim= embedding_matrix.shape[0],\n",
    "                            output_dim= embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix], \n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(16))    \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=0, mode='auto')\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(new_word_index[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "#         new_doc_arr = np.array(new_doc).reshape(1, max_doc_word_length)\n",
    "        new_doc_arr = np.array(new_doc)\n",
    "        new_corpus.append( new_doc_arr)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = text_to_index(train_df_sample.text)\n",
    "X_train = pad_sequences(X_train_texts, maxlen= max_doc_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 <class 'numpy.ndarray'>\n",
      "200 <class 'numpy.ndarray'>\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# X_train_texts_ = list(X_train_texts)\n",
    "print(len(X_train), type(X_train))\n",
    "print(len(X_train[0]), type(X_train[0]))\n",
    "print(X_train[0].shape)\n",
    "print(X_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2016 - acc: 0.5762 - val_loss: 1.8308 - val_acc: 0.4078\n",
      "Epoch 2/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1761 - acc: 0.5890 - val_loss: 1.8554 - val_acc: 0.4067\n",
      "Epoch 3/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1733 - acc: 0.5863 - val_loss: 1.8350 - val_acc: 0.4156\n",
      "Epoch 4/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1680 - acc: 0.5883 - val_loss: 1.8444 - val_acc: 0.4089\n",
      "Epoch 5/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1660 - acc: 0.5919 - val_loss: 1.8465 - val_acc: 0.4100\n",
      "Epoch 6/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1612 - acc: 0.5930 - val_loss: 1.8448 - val_acc: 0.4122\n",
      "Epoch 7/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1598 - acc: 0.5962 - val_loss: 1.8428 - val_acc: 0.4078\n",
      "Epoch 8/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1579 - acc: 0.5926 - val_loss: 1.8475 - val_acc: 0.4122\n",
      "Epoch 9/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1617 - acc: 0.5920 - val_loss: 1.8597 - val_acc: 0.4033\n",
      "Epoch 10/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1618 - acc: 0.5938 - val_loss: 1.8423 - val_acc: 0.4122\n",
      "Epoch 11/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1654 - acc: 0.5932 - val_loss: 1.8420 - val_acc: 0.4056\n",
      "Epoch 12/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1644 - acc: 0.5890 - val_loss: 1.8530 - val_acc: 0.4100\n",
      "Epoch 13/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1651 - acc: 0.5917 - val_loss: 1.8478 - val_acc: 0.4156\n",
      "Epoch 14/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1692 - acc: 0.5877 - val_loss: 1.8638 - val_acc: 0.4078\n",
      "Epoch 15/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1638 - acc: 0.5949 - val_loss: 1.8489 - val_acc: 0.4078\n",
      "Epoch 16/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1584 - acc: 0.5964 - val_loss: 1.8514 - val_acc: 0.4144\n",
      "Epoch 17/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1651 - acc: 0.5907 - val_loss: 1.8539 - val_acc: 0.4011\n",
      "Epoch 18/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1552 - acc: 0.5970 - val_loss: 1.8522 - val_acc: 0.4056\n",
      "Epoch 19/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1581 - acc: 0.5942 - val_loss: 1.8482 - val_acc: 0.4122\n",
      "Epoch 20/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1597 - acc: 0.5951 - val_loss: 1.8697 - val_acc: 0.4089\n",
      "Epoch 21/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1648 - acc: 0.5896 - val_loss: 1.8486 - val_acc: 0.4067\n",
      "Epoch 22/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1591 - acc: 0.5920 - val_loss: 1.8559 - val_acc: 0.4122\n",
      "Epoch 23/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1573 - acc: 0.5991 - val_loss: 1.8507 - val_acc: 0.4078\n",
      "Epoch 24/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1579 - acc: 0.5946 - val_loss: 1.8498 - val_acc: 0.4211\n",
      "Epoch 25/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1575 - acc: 0.5949 - val_loss: 1.8735 - val_acc: 0.4056\n",
      "Epoch 26/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1650 - acc: 0.5920 - val_loss: 1.8523 - val_acc: 0.4011\n",
      "Epoch 27/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1597 - acc: 0.5953 - val_loss: 1.8539 - val_acc: 0.4156\n",
      "Epoch 28/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1570 - acc: 0.5956 - val_loss: 1.8684 - val_acc: 0.4078\n",
      "Epoch 29/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1500 - acc: 0.5979 - val_loss: 1.8561 - val_acc: 0.4100\n",
      "Epoch 30/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1459 - acc: 0.5991 - val_loss: 1.8576 - val_acc: 0.4111\n",
      "Epoch 31/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1501 - acc: 0.5980 - val_loss: 1.8652 - val_acc: 0.4122\n",
      "Epoch 32/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1492 - acc: 0.5985 - val_loss: 1.8717 - val_acc: 0.4067\n",
      "Epoch 33/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1600 - acc: 0.5916 - val_loss: 1.8744 - val_acc: 0.4044\n",
      "Epoch 34/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1557 - acc: 0.5956 - val_loss: 1.8658 - val_acc: 0.4100\n",
      "Epoch 35/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1525 - acc: 0.5980 - val_loss: 1.8860 - val_acc: 0.4056\n",
      "Epoch 36/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1531 - acc: 0.5962 - val_loss: 1.8691 - val_acc: 0.4133\n",
      "Epoch 37/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1548 - acc: 0.5923 - val_loss: 1.8856 - val_acc: 0.4011\n",
      "Epoch 38/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1586 - acc: 0.5947 - val_loss: 1.8575 - val_acc: 0.4089\n",
      "Epoch 39/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1546 - acc: 0.5932 - val_loss: 1.8571 - val_acc: 0.4111\n",
      "Epoch 40/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1464 - acc: 0.5959 - val_loss: 1.8693 - val_acc: 0.4122\n",
      "Epoch 41/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1473 - acc: 0.5995 - val_loss: 1.8584 - val_acc: 0.4144\n",
      "Epoch 42/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1473 - acc: 0.5981 - val_loss: 1.8628 - val_acc: 0.4000\n",
      "Epoch 43/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1448 - acc: 0.6002 - val_loss: 1.8596 - val_acc: 0.4167\n",
      "Epoch 44/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1431 - acc: 0.6000 - val_loss: 1.8732 - val_acc: 0.4067\n",
      "Epoch 45/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1432 - acc: 0.6017 - val_loss: 1.8628 - val_acc: 0.3989\n",
      "Epoch 46/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1385 - acc: 0.6004 - val_loss: 1.8637 - val_acc: 0.4089\n",
      "Epoch 47/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1343 - acc: 0.6025 - val_loss: 1.8667 - val_acc: 0.4022\n",
      "Epoch 48/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1340 - acc: 0.6038 - val_loss: 1.8756 - val_acc: 0.4044\n",
      "Epoch 49/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1395 - acc: 0.6019 - val_loss: 1.8669 - val_acc: 0.4122\n",
      "Epoch 50/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1354 - acc: 0.6064 - val_loss: 1.8660 - val_acc: 0.4189\n",
      "Epoch 51/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1380 - acc: 0.5990 - val_loss: 1.8698 - val_acc: 0.4044\n",
      "Epoch 52/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1414 - acc: 0.6015 - val_loss: 1.8780 - val_acc: 0.4100\n",
      "Epoch 53/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1457 - acc: 0.5981 - val_loss: 1.8688 - val_acc: 0.4200\n",
      "Epoch 54/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.1415 - acc: 0.6035 - val_loss: 1.8837 - val_acc: 0.4044\n",
      "Epoch 55/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1406 - acc: 0.5999 - val_loss: 1.8726 - val_acc: 0.4156\n",
      "Epoch 56/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1411 - acc: 0.5983 - val_loss: 1.8752 - val_acc: 0.4089\n",
      "Epoch 57/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 1.1366 - acc: 0.6030 - val_loss: 1.8912 - val_acc: 0.4100\n",
      "Epoch 58/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 1.1327 - acc: 0.6021 - val_loss: 1.9007 - val_acc: 0.4100\n",
      "Epoch 59/100\n",
      "8100/8100 [==============================] - 16s 2ms/step - loss: 1.1415 - acc: 0.5985 - val_loss: 1.8853 - val_acc: 0.4189\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.1707 - acc: 0.5852 - val_loss: 1.9230 - val_acc: 0.4011\n",
      "Epoch 61/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1662 - acc: 0.5858 - val_loss: 1.8836 - val_acc: 0.4089\n",
      "Epoch 62/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1450 - acc: 0.5979 - val_loss: 1.8979 - val_acc: 0.4100\n",
      "Epoch 63/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1425 - acc: 0.5995 - val_loss: 1.8916 - val_acc: 0.4067\n",
      "Epoch 64/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1333 - acc: 0.6006 - val_loss: 1.8843 - val_acc: 0.4156\n",
      "Epoch 65/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1327 - acc: 0.6025 - val_loss: 1.8927 - val_acc: 0.4067\n",
      "Epoch 66/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1265 - acc: 0.6058 - val_loss: 1.8723 - val_acc: 0.4133\n",
      "Epoch 67/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1233 - acc: 0.6063 - val_loss: 1.8970 - val_acc: 0.4044\n",
      "Epoch 68/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1251 - acc: 0.6051 - val_loss: 1.8759 - val_acc: 0.4133\n",
      "Epoch 69/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1247 - acc: 0.6079 - val_loss: 1.8794 - val_acc: 0.4078\n",
      "Epoch 70/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1286 - acc: 0.6064 - val_loss: 1.8751 - val_acc: 0.4133\n",
      "Epoch 71/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.1243 - acc: 0.6059 - val_loss: 1.8802 - val_acc: 0.4033\n",
      "Epoch 72/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1224 - acc: 0.6070 - val_loss: 1.8866 - val_acc: 0.4089\n",
      "Epoch 73/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1187 - acc: 0.6100 - val_loss: 1.8814 - val_acc: 0.4133\n",
      "Epoch 74/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1175 - acc: 0.6115 - val_loss: 1.8814 - val_acc: 0.4144\n",
      "Epoch 75/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1213 - acc: 0.6083 - val_loss: 1.8872 - val_acc: 0.4033\n",
      "Epoch 76/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1226 - acc: 0.6053 - val_loss: 1.8853 - val_acc: 0.4022\n",
      "Epoch 77/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1214 - acc: 0.6083 - val_loss: 1.8819 - val_acc: 0.4111\n",
      "Epoch 78/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1218 - acc: 0.6074 - val_loss: 1.9090 - val_acc: 0.4044\n",
      "Epoch 79/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1228 - acc: 0.6070 - val_loss: 1.8939 - val_acc: 0.4111\n",
      "Epoch 80/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1154 - acc: 0.6121 - val_loss: 1.8866 - val_acc: 0.4067\n",
      "Epoch 81/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1139 - acc: 0.6079 - val_loss: 1.8841 - val_acc: 0.4111\n",
      "Epoch 82/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1137 - acc: 0.6100 - val_loss: 1.8884 - val_acc: 0.4122\n",
      "Epoch 83/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1156 - acc: 0.6075 - val_loss: 1.8870 - val_acc: 0.4122\n",
      "Epoch 84/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1146 - acc: 0.6077 - val_loss: 1.9209 - val_acc: 0.3978\n",
      "Epoch 85/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1276 - acc: 0.6037 - val_loss: 1.8898 - val_acc: 0.4144\n",
      "Epoch 86/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1252 - acc: 0.6040 - val_loss: 1.9173 - val_acc: 0.4067\n",
      "Epoch 87/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1229 - acc: 0.6062 - val_loss: 1.8826 - val_acc: 0.4144\n",
      "Epoch 88/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1191 - acc: 0.6093 - val_loss: 1.8899 - val_acc: 0.4078\n",
      "Epoch 89/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1234 - acc: 0.6038 - val_loss: 1.9075 - val_acc: 0.4067\n",
      "Epoch 90/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1181 - acc: 0.6079 - val_loss: 1.8900 - val_acc: 0.4111\n",
      "Epoch 91/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1170 - acc: 0.6067 - val_loss: 1.9160 - val_acc: 0.4056\n",
      "Epoch 92/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1245 - acc: 0.6047 - val_loss: 1.8929 - val_acc: 0.4200\n",
      "Epoch 93/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.1179 - acc: 0.6059 - val_loss: 1.9103 - val_acc: 0.4044\n",
      "Epoch 94/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1160 - acc: 0.6078 - val_loss: 1.9092 - val_acc: 0.4067\n",
      "Epoch 95/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1204 - acc: 0.6069 - val_loss: 1.9232 - val_acc: 0.4056\n",
      "Epoch 96/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.1215 - acc: 0.6037 - val_loss: 1.9212 - val_acc: 0.4044\n",
      "Epoch 97/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1223 - acc: 0.6057 - val_loss: 1.9246 - val_acc: 0.4111\n",
      "Epoch 98/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1263 - acc: 0.6059 - val_loss: 1.9294 - val_acc: 0.4056\n",
      "Epoch 99/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1147 - acc: 0.6080 - val_loss: 1.9055 - val_acc: 0.4133\n",
      "Epoch 100/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1139 - acc: 0.6094 - val_loss: 1.9303 - val_acc: 0.4056\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = label_list, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size= 4050,\n",
    "                    epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func_name = 'categorical_crossentropy'\n",
    "# for value in history.history[\"categorical_crossentropy\"]:\n",
    "#     print(history)\n",
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate the model\n",
    "# loss_accuracy = model.evaluate(sequences1[0:200], label_list[0:200], verbose=1)\n",
    "# print(type(loss_accuracy), loss_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tokenizer = Tokenizer()\n",
    "# test_tokenizer.fit_on_texts(test_texts)\n",
    "# test_sequences = test_tokenizer.texts_to_sequences(test_texts)\n",
    "# test_sequences1 = pad_sequences(test_sequences, maxlen=max_doc_word_length, padding='post')\n",
    "# X_train_texts = text_to_index(train_df_sample.text)\n",
    "# X_train = pad_sequences(X_train_texts, maxlen= max_doc_word_length)\n",
    "\n",
    "Y_sequences1 = text_to_index(test_pickle_df.text)\n",
    "\n",
    "Y_test = pad_sequences(Y_sequences1, maxlen=max_doc_word_length)\n",
    "print(Y_test[732])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_res = model.predict(Y_test[0:], verbose=1)\n",
    "# print(len(predict_res), predict_res)\n",
    "\n",
    "final_res = []\n",
    "for pre_res in predict_res:\n",
    "    final_res.append(np.argmax(pre_res))\n",
    "\n",
    "    \n",
    "# 'Japan_Travel': 0, 'KR_ENTERTAIN' 娱乐: 1, 'Makeup' 化妆 : 2, 'Tech_Job':  3, 'WomenTalk': 4, \n",
    "# 'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "# 7, 9, 4, 5,    1/2, 1/8, 3/7, 1, 7, 0\n",
    "# 0, 9, 6, x(生活), 9,  9, 9, 7/3, 7, 1\n",
    "print(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "print(len(final_res))\n",
    "result_txt = \"result001\" + \".txt\"\n",
    "ids = 0\n",
    "with open(result_txt, 'w') as out:\n",
    "    out.write(\"id,category\" + '\\n')\n",
    "    for value in final_res:\n",
    "        out.write(str(ids) + \",\" + str(value) + '\\n')\n",
    "        ids += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
