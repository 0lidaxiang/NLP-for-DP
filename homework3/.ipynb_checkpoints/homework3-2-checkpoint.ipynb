{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 613,
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
=======
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "from keras import metrics\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN,Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from datetime import datetime\n",
<<<<<<< HEAD
    "from gensim.models import word2vec\n",
    "import pandas as pd"
=======
    "from gensim.models import word2vec"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts data\n",
=======
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11199 11199.666666666666\n"
     ]
    }
   ],
   "source": [
    "input_size = int(33599 / 3)\n",
    "print(input_size, 33599 / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ['Tech_Job', 'e-shopping', 'babymother', 'joke', 'Makeup', 'Japan_Travel', 'WomenTalk', 'movie', 'KR_ENTERTAIN', 'graduate']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
    "TRAINING_PATH = './data/training/'\n",
    "TESTING_PATH = './data/testing/'\n",
    "\n",
    "categories = [dirname for dirname in os.listdir(TRAINING_PATH) if dirname[-4:] != '_cut']\n",
<<<<<<< HEAD
    "# print(len(categories), str(categories))\n",
=======
    "print(len(categories), str(categories))\n",
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
    "\n",
    "category2idx = {'Japan_Travel': 0, 'KR_ENTERTAIN': 1, 'Makeup': 2, 'Tech_Job':  3, 'WomenTalk': 4,\n",
    "                  'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "\n",
<<<<<<< HEAD
    "train_pickle_df = pd.read_pickle('train.pkl')\n",
    "train_texts = train_pickle_df[\"text\"].values\n",
    "train_labels = train_pickle_df[\"category\"]\n",
    "# print(len(train_texts), train_texts[0], train_labels[0])   \n",
    "\n",
    "test_pickle_df = pd.read_pickle('test.pkl')\n",
    "test_texts = test_pickle_df[\"text\"].values\n",
    "# print(len(test_texts), test_texts[0]) "
=======
    "\n",
    "train_list = []\n",
    "for category in categories:\n",
    "    category_idx = category2idx[category]\n",
    "    category_path = TRAINING_PATH + category + '_cut/'\n",
    "    \n",
    "    for filename in os.listdir(category_path):\n",
    "        filepath = category_path + filename\n",
    "        \n",
    "        with open(filepath, encoding='utf-8') as file:\n",
    "            words = file.read().strip().split(' / ')\n",
    "            train_list.append([words, category_idx])\n",
    "            "
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 9000 3\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "# # process some data\n",
    "train_labels_list = list(train_labels)\n",
    "print(type(train_labels_list), len(train_labels_list), train_labels_list[0])\n",
    "embedding_matrix_len = len(train_labels_list)\n",
    "\n",
    "label_id = 0\n",
    "label_list = np.zeros((embedding_matrix_len, 10))\n",
    "for label_val in train_labels_list:\n",
    "    index = label_val - 1\n",
    "    label_list[label_id][index] = 1\n",
    "    label_id += 1\n",
    "print(label_list[0])\n",
    "print(len(label_list)) "
=======
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sss in train_list:\n",
    "#     if sss[1] == 5:\n",
    "#         print(sss)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "category_path = TESTING_PATH \n",
    "    \n",
    "for filename in os.listdir(category_path):\n",
    "        filepath = category_path + filename\n",
    "        \n",
    "        with open(filepath, encoding='utf-8') as file:\n",
    "            words = file.read().strip().split(' / ')\n",
    "            test_list.append([words, 1])\n",
    "\n",
    "# test_list[1]"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 69,
=======
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list[0][1]\n",
    "train_dict =  dict(enumerate(train_list))\n",
    "test_dict =  dict(enumerate(test_list))\n",
    "# train_dict[0][1]\n",
    "# len(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getArticleVec(article):\n",
    "#     article_vec = []\n",
    "#     length = 0\n",
    "#     for word in article[0]:\n",
    "#         if word in wvv.keys():\n",
    "# #             print(answer[word])\n",
    "#             article_vec.append(answer[word])\n",
    "#             length+=1\n",
    "#         else:\n",
    "#             temp = np.zeros(250)\n",
    "#             article_vec.append(temp)\n",
    "# #     print(len(article_vec), article_vec[0], article_vec)\n",
    "#     return np.mean(article_vec ,axis=0)\n",
    "\n",
    "# res1 = getArticleVec(train_list[0])\n",
    "# # print(res1)\n",
    "# print(len(res1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = np.zeros((len(train_dict), 10))\n",
    "# # inss = 0\n",
    "# for i,article in train_dict.items():\n",
    "# #     print(i,article[1])\n",
    "# #     if inss > 10:\n",
    "# #         break\n",
    "#     sec_i = article[1]\n",
    "#     label_list[i][sec_i] = 1\n",
    "#     print(label_list[i])\n",
    "\n",
    "# #     inss += 1\n",
    "# print(len(label_list))\n",
    "\n",
    "# label_new_list= np.zeros((len(train_dict), 10))\n",
    "# label_list_i = 0\n",
    "# for label in label_list:\n",
    "# #     label_list[label_list_i] = np.reshape( label_list[label_list_i] ,(1, 10))\n",
    "    \n",
    "#     label_new = np.reshape(label_list[label_list_i],(1, 10))\n",
    "# #     print(label_new.shape)\n",
    "# #     label_list[label_list_i] = label_new\n",
    "# #     print(label_list[label_list_i].shape)\n",
    "#     label_new_list[label_list_i] = label_new\n",
    "#     label_list_i += 1\n",
    "    \n",
    "# print(label_new_list.shape)\n",
    "# print(label_new_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "['櫻花林', '好比', '考科', '床上', '一點現', '記住', '寶寶的', '柔嫩', '不規則', '朴智妍']\n"
=======
      "<class 'gensim.models.word2vec.Word2Vec'>\n"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# get word embedding vector\n",
    "answer = word2vec.Word2Vec.load(\"word2vec1.model\")\n",
    "# print(type(answer))\n",
=======
    "answer = word2vec.Word2Vec.load(\"word2vec1.model\")\n",
    "print(type(answer))\n",
    "\n",
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "wvv_keys = wvv.keys()\n",
    "wvv_keys_list = list(wvv_keys)\n",
<<<<<<< HEAD
    "# print(wvv_keys_list[:10]) #['櫻花林', '好比', '考科', '床上', '一點現', '記住', '寶寶的', '柔嫩', '不規則', '朴智妍']\n",
    "# wvv_new_dict = dict(enumerate(wvv_keys_list))\n",
    "# # word_index = len(wvv_keys_list)\n",
    "# print(wvv_new_dict)"
=======
    "wvv_new_dict = dict(enumerate(wvv_keys_list))\n",
    "# word_index = len(wvv_keys_list)"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pickle_df = pd.read_pickle('train.pkl')\n",
    "train_texts = train_pickle_df[\"text\"]\n",
    "train_texts = train_texts.values\n",
    "# print(train_texts)\n",
    "# ss = type(pickle_df[\"text\"])\n",
    "# print(ss)\n",
    "# print(type(val))\n",
    "# print(val[0])\n",
    "# print(val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Found 232236 unique tokens\n"
=======
      "Found 232236 unique tokens\n",
      "Found 0 word vectors.\n"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "max_doc_word_length = max(len(l) for l in train_texts)\n",
    "sequences1 = pad_sequences(sequences, maxlen=max_doc_word_length, padding='post')\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "# data = pad_sequences(sequences)\n",
    "# print(\"Shape of data tensor:\" , data.shape)"
=======
    "texts = train_texts\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "# data = pad_sequences(sequences)\n",
    "# print(\"Shape of data tensor:\" , data.shape)\n",
    "\n",
    "# path = \"word_model.model\"\n",
    "# model.gensim.models.Word2Vec.load(path)\n",
    "embeddings_index = dict()\n",
    "MAX_NB_WORDS = len(word_index)\n",
    "# word_vectors = answer.wv\n",
    "MAX_NB_WORDS = len(word_index)\n",
    "for word,vocab_obj in answer.wv.vocab.items():\n",
    "    if int(vocab_obj.index) < MAX_NB_WORDS and word == -1:\n",
    "#     print(word,vocab_obj)\n",
    "        embeddings_index[word] = word_vectors[word]\n",
    "# del answer, word_vectors\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "OUT_SIZE = 250 \n",
    "embedding_matrix = np.zeros((len(word_index) + 1, OUT_SIZE))\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        embedding_matrix[i]=np.reshape(embedding_matrix[i],(1, 250))\n",
    "embedding_layer = Embedding(len(word_index) + 1, OUT_SIZE,\n",
    "                            weights = [embedding_matrix],\n",
    "                            input_length = 250,\n",
    "                            trainable = False)"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
=======
   "execution_count": 625,
   "metadata": {},
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "3971 [   45 40836  2384 ...     0     0     0]\n"
=======
      "232237\n"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# print(len(sequences))\n",
    "print(len(sequences1[0]), sequences1[0])\n",
    "# print(len(sequences1[0]))\n",
    "# print(wvv) #'同上': <gensim.models.keyedvectors.Vocab object at 0x7fd385b03d68>, \n",
    "# print(type(wvv)) <class 'dict'>"
=======
    "embedding_matrix_len = len(embedding_matrix)\n",
    "label_list = np.zeros((embedding_matrix_len, 10))\n",
    "label_id = 0\n",
    "for i,article in train_dict.items():\n",
    "    article_len = len(article)\n",
    "    sec_i = article[1]\n",
    "\n",
    "    for w_i in range(0, article_len):\n",
    "        label_list[label_id][sec_i] = 1\n",
    "        label_id += 1\n",
    "#             break\n",
    "#         print(label_list[i])\n",
    "    \n",
    "# label_new_list= np.zeros((embedding_matrix_len, 10))\n",
    "# label_list_i = 0\n",
    "# for label in label_list:\n",
    "#     label_new = np.reshape(label_list[label_list_i],(1, 10))\n",
    "#     label_new_list[label_list_i] = label_new\n",
    "#     label_list_i += 1\n",
    "print(len(label_list))    \n",
    "# del label_list    \n",
    "# print(label_list.shape)\n",
    "# print(label_new_list[0].shape)"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 250))\n",
    "for word, i in word_index.items():\n",
    "    if word in wvv_keys_list:\n",
    "        embedding_vector = answer[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrix[0]))\n",
    "# input_array = np.random.randint(embedding_matrix_len, size=(50, 3971))\n",
    "# print(input_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 626,
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
<<<<<<< HEAD
      "embedding_6 (Embedding)      (None, 3971, 250)         58059250  \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
=======
      "embedding_40 (Embedding)     (None, 250, 250)          58059250  \n",
      "_________________________________________________________________\n",
      "simple_rnn_43 (SimpleRNN)    (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 10)                0         \n",
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
      "=================================================================\n",
      "Total params: 58,074,810\n",
      "Trainable params: 15,560\n",
      "Non-trainable params: 58,059,250\n",
<<<<<<< HEAD
      "_________________________________________________________________\n"
=======
      "_________________________________________________________________\n",
      "0  , loss :  0.948938    acc :  0.12256958335638046   minutes:  0.29\n"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# input_dim = 400\n",
    "# output_dim = 100\n",
    "embedding_layer = Embedding(vocab_size, 250, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length= max_doc_word_length, \n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(SimpleRNN(input_dim = 250, output_dim = 50, unroll=True))\n",
    "nb_classes = 10\n",
    "model.add(Dense( nb_classes, input_dim = 3971))\n",
    "model.add(Activation('softmax'))\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=0, mode='auto')\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "model.summary()"
=======
    "\n",
    "# input_dim = 400\n",
    "# output_dim = 100\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(SimpleRNN(output_dim = 50, unroll=True))\n",
    "nb_classes = 10\n",
    "model.add(Dense( nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=0, mode='auto')\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=[metrics.mae, metrics.categorical_accuracy])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "start = 0\n",
    "\n",
    "for embed_item_index in range(0, 487):\n",
    "#     train_result= np.asarray([embedding_matrix])\n",
    "#     train_label =  np.asarray([label_list[label_id]])\n",
    "    \n",
    "    end = start + 100\n",
    "    train_result= embedding_matrix[start:end]\n",
    "    train_label = label_list[start:end]\n",
    "    history = model.fit(x = train_result, y = train_label, validation_split=0.1, callbacks=[early_stopping], epochs = 5, verbose = 0)\n",
    "    model.save('my_model.h5')\n",
    "    scores = model.evaluate(x = train_result, y = train_label, batch_size=32, verbose = 0)\n",
    "    if  embed_item_index % 10 == 0 :  \n",
    "        end_time = time.time()\n",
    "        excution_time = end_time- start_time\n",
    "        excution_time_min = round(excution_time / 60, 2)\n",
    "        print(embed_item_index, \" , loss : \", round(scores[0], 6), \"   acc : \" , scores[1],\n",
    "              \"  minutes: \" , excution_time_min)\n",
    "#     loss_func_name = 'categorical_crossentropy'\n",
    "# #     for value in history.history[loss_func_name]:\n",
    "#     print(history)\n",
    "        \n",
    "    if embed_item_index < 4 :  \n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        break\n",
    "    start += 100\n"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/1\n",
      "8100/8100 [==============================] - 339s 42ms/step - loss: 2.2302 - mean_absolute_error: 0.1784 - categorical_accuracy: 0.1047 - val_loss: 5.6647 - val_mean_absolute_error: 0.1993 - val_categorical_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = sequences1, y = label_list, \n",
    "                    validation_split=0.1, \n",
    "                    callbacks=[early_stopping], \n",
    "                    epochs = 1, verbose = 0)"
=======
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "model = load_model('my_model.h5')\n"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 118,
=======
   "execution_count": 630,
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "100/100 [==============================] - 2s 16ms/step\n",
      "[2.1838293075561523, 0.1774781048297882, 0.0]\n"
=======
      "1 [[5.3843707e-03 1.4126182e-03 1.1725756e-03 9.8094708e-01 2.5647839e-03\n",
      "  1.7822894e-03 1.4089271e-03 1.0094179e-03 9.2199259e-04 3.3960063e-03]]\n",
      "[3]\n"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "loss_and_metrics = model.evaluate(sequences1[0:100], label_list[0:100], verbose=1)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer = Tokenizer()\n",
    "test_tokenizer.fit_on_texts(test_texts)\n",
    "test_sequences = test_tokenizer.texts_to_sequences(test_texts)\n",
    "test_sequences1 = pad_sequences(test_sequences, maxlen=max_doc_word_length, padding='post')"
=======
    "test_feature=  embedding_matrix[9000:9001]\n",
    "predict_res = model.predict(test_feature, batch_size= 32, verbose=0)\n",
    "print(len(predict_res), predict_res)\n",
    "\n",
    "final_res = []\n",
    "now_item = predict_res[0]\n",
    "max_val = max(now_item)\n",
    "final_res.append(np.argmax(now_item))\n",
    "print(final_res)"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 123,
=======
   "execution_count": 629,
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "1000/1000 [==============================] - 17s 17ms/step\n",
      "1000 [[0.10809825 0.09540619 0.11260946 ... 0.114498   0.1104389  0.11035634]\n",
      " [0.10809823 0.09540614 0.11260948 ... 0.11449803 0.11043893 0.11035636]\n",
      " [0.10809824 0.09540614 0.11260945 ... 0.11449799 0.1104389  0.11035635]\n",
      " ...\n",
      " [0.10809822 0.09540616 0.11260947 ... 0.11449805 0.11043893 0.11035637]\n",
      " [0.10809824 0.09540617 0.11260946 ... 0.11449803 0.11043891 0.11035635]\n",
      " [0.10809825 0.09540617 0.11260945 ... 0.114498   0.11043891 0.11035633]]\n"
     ]
    }
   ],
   "source": [
    "# test_feature= sequences1[4000:4002]\n",
    "predict_res = model.predict(test_sequences1, verbose=1)\n",
    "print(len(predict_res), predict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
=======
      "1001\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected embedding_40_input to have shape (250,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-629-04eeb39f6eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpredicc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_embedding_matrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_feature\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpredict_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print(len(predict_res), predict_res[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected embedding_40_input to have shape (250,) but got array with shape (1,)"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
     ]
    }
   ],
   "source": [
    "final_res = []\n",
<<<<<<< HEAD
    "for pre_res in predict_res:\n",
    "    final_res.append(np.argmax(pre_res))\n",
    "print(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# model = load_model('my_model.h5')\n",
    "# len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_res = []\n",
    "# print(len(test_embedding_matrix))\n",
    "# for predicc in test_embedding_matrix:\n",
    "#     test_feature= np.asarray(predicc)\n",
    "#     predict_res = model.predict(test_feature, batch_size= 32, verbose=0)\n",
    "# #     print(len(predict_res), predict_res[0])\n",
    "    \n",
    "#     now_item = predict_res[0]\n",
    "#     max_val = max(now_item)\n",
    "#     final_res.append(np.argmax(now_item))\n",
    "# print(final_res)"
=======
    "print(len(test_embedding_matrix))\n",
    "for predicc in test_embedding_matrix:\n",
    "    test_feature= np.asarray(predicc)\n",
    "    predict_res = model.predict(test_feature, batch_size= 32, verbose=0)\n",
    "#     print(len(predict_res), predict_res[0])\n",
    "    \n",
    "    now_item = predict_res[0]\n",
    "    max_val = max(now_item)\n",
    "    final_res.append(np.argmax(now_item))\n",
    "print(final_res)"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# # result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "# print(len(final_res))\n",
    "# result_txt = \"result001\" + \".txt\"\n",
    "# ids = 0\n",
    "# with open(result_txt, 'a') as out:\n",
    "#     out.write(\"id,category\" + '\\n')\n",
    "#     for value in final_res:\n",
    "#         out.write(str(ids) + \",\" + str(value) + '\\n')\n",
    "#         ids += 1"
=======
    "# result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "print(len(final_res))\n",
    "result_txt = \"result001\" + \".txt\"\n",
    "ids = 0\n",
    "with open(result_txt, 'a') as out:\n",
    "    out.write(\"id,category\" + '\\n')\n",
    "    for value in final_res:\n",
    "        out.write(str(ids) + \",\" + str(value) + '\\n')\n",
    "        ids += 1"
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "# start_time = time.time()\n",
    "# start = 0\n",
    "\n",
    "# for embed_item_index in range(0, 487):\n",
    "# #     train_result= np.asarray([embedding_matrix])\n",
    "# #     train_label =  np.asarray([label_list[label_id]])\n",
    "    \n",
    "#     end = start + 100\n",
    "#     train_result= articles_matrix[start:end]\n",
    "#     train_label = label_list[start:end]\n",
    "#     history = model.fit(x = train_result, y = train_label, validation_split=0.1, callbacks=[early_stopping], epochs = 5, verbose = 0)\n",
    "#     model.save('my_model.h5')\n",
    "#     scores = model.evaluate(x = train_result, y = train_label, batch_size=32, verbose = 0)\n",
    "#     if  embed_item_index % 10 == 0 :  \n",
    "#         end_time = time.time()\n",
    "#         excution_time = end_time- start_time\n",
    "#         excution_time_min = round(excution_time / 60, 2)\n",
    "#         print(embed_item_index, \" , loss : \", round(scores[0], 6), \"   acc : \" , scores[1],\n",
    "#               \"  minutes: \" , excution_time_min)\n",
    "# #     loss_func_name = 'categorical_crossentropy'\n",
    "# # #     for value in history.history[loss_func_name]:\n",
    "# #     print(history)\n",
    "        \n",
    "#     if embed_item_index < 40 :  \n",
    "#         pass\n",
    "        \n",
    "#     else:\n",
    "#         break\n",
    "#     start += 100"
   ]
=======
   "source": []
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
