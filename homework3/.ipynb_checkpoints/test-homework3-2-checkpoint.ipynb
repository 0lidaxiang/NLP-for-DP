{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.word2vec.Word2Vec'>\n",
      "<class 'dict'> 36132\n",
      "[<gensim.models.keyedvectors.Vocab object at 0x7f62c0bf44a8>, <gensim.models.keyedvectors.Vocab object at 0x7f62c0b22208>, <gensim.models.keyedvectors.Vocab object at 0x7f62c0fd2668>, <gensim.models.keyedvectors.Vocab object at 0x7f62c0c0a4e0>, <gensim.models.keyedvectors.Vocab object at 0x7f62c0e35dd8>, <gensim.models.keyedvectors.Vocab object at 0x7f62c0bf0898>, <gensim.models.keyedvectors.Vocab object at 0x7f62c09e1be0>, <gensim.models.keyedvectors.Vocab object at 0x7f62c0a5d2e8>, <gensim.models.keyedvectors.Vocab object at 0x7f62c1036a20>, <gensim.models.keyedvectors.Vocab object at 0x7f62c0ecc5c0>]\n",
      "<class 'dict_keys'> 36132\n",
      "['七天', '什麼連', '背後都', '顏色畫', '半面', '開口', '我化妝', '自知之明', '控器', '櫻不過']\n",
      "<class 'numpy.ndarray'> 250 [ 1.29469201e-01  9.93388519e-02 -3.01652849e-01  1.69350415e-01\n",
      " -6.25951365e-02 -2.05337226e-01  1.95295170e-01  3.59804370e-02\n",
      "  1.43934846e-01 -1.96315851e-02  2.31693909e-01  1.51878893e-01\n",
      " -6.19187625e-03  6.11272454e-02  1.14445522e-01  1.70429647e-01\n",
      " -5.24917357e-02  2.33712524e-01  8.56012404e-02 -1.08685710e-01\n",
      " -4.85128053e-02 -2.07923800e-01 -1.89773235e-02  2.40019977e-01\n",
      " -1.23314731e-01 -2.01177895e-01 -6.57802884e-05  1.90735236e-01\n",
      " -1.08382516e-01  2.17012972e-01  3.08904916e-01 -6.27809912e-02\n",
      "  6.27423972e-02  1.20317027e-01 -2.15248764e-01  1.81609437e-01\n",
      "  2.16244474e-01  4.99928407e-02 -3.93563300e-01 -4.64095622e-02\n",
      " -1.62380096e-02  1.39031515e-01  1.28462642e-01 -9.49258804e-02\n",
      "  4.15007383e-01  2.30861694e-01 -1.25385642e-01  1.99323118e-01\n",
      "  1.78577051e-01  5.97154573e-02 -4.48642634e-02 -8.91068503e-02\n",
      " -1.89909697e-01 -8.46233144e-02 -2.83460140e-01  1.88968107e-01\n",
      " -9.53775421e-02 -1.21669397e-01 -1.91831291e-02 -1.23217598e-01\n",
      " -1.07107975e-01 -7.45946029e-03 -6.36760890e-02 -1.26924112e-01\n",
      " -1.59748774e-02  4.89436090e-02 -3.23105514e-01 -1.24020102e-02\n",
      "  5.50881773e-02  1.49993241e-01  1.20139569e-01 -1.03860825e-01\n",
      "  1.18537091e-01  1.62843019e-01  2.34648317e-01 -9.68808308e-02\n",
      " -5.00181377e-01 -4.62972000e-02 -1.53249606e-01  7.99368694e-02\n",
      "  2.35070307e-02 -2.32048873e-02  1.41495883e-01  2.49785736e-01\n",
      " -1.37228489e-01  2.15989202e-01  9.12511870e-02  8.24414641e-02\n",
      "  1.55627728e-01  2.13630334e-01  7.61274472e-02  5.29262841e-01\n",
      "  8.44442397e-02 -1.00403123e-01 -1.35866553e-02 -3.33626606e-02\n",
      "  3.79238203e-02  3.21364067e-02 -1.26460999e-01 -3.93217914e-02\n",
      " -7.74387568e-02 -2.68434852e-01  9.59265307e-02 -2.71660209e-01\n",
      "  1.09322444e-02  4.82006185e-02 -2.62118995e-01 -3.62688541e-01\n",
      " -1.36800501e-02 -2.97197640e-01 -1.12526394e-01 -8.52443576e-02\n",
      "  2.00894792e-02  2.84519196e-01 -5.23579866e-02 -8.98607671e-02\n",
      "  1.29342824e-01 -6.13706419e-03  1.65639992e-03  1.26972139e-01\n",
      " -2.41704091e-01 -1.28131166e-01  1.42018154e-01 -3.01250875e-01\n",
      "  1.06497332e-01  2.04113692e-01  9.75085348e-02  6.43170774e-02\n",
      "  2.33548045e-01  1.25122488e-01  6.40095770e-02  3.77085572e-03\n",
      "  5.99669404e-02  1.23343863e-01 -1.00751095e-01  6.75594583e-02\n",
      "  3.55454087e-01 -1.43921778e-01 -4.72316332e-02 -8.03941190e-02\n",
      " -4.10629123e-01  8.57796296e-02 -8.53954628e-02 -8.75380188e-02\n",
      "  7.73595721e-02  1.20463878e-01  1.26184702e-01  6.55628666e-02\n",
      " -1.66915879e-01 -1.74005195e-01 -4.70497571e-02 -2.25662678e-01\n",
      "  3.88340689e-02 -2.66749799e-01  1.17071934e-01  1.80722371e-01\n",
      " -4.73724574e-01 -3.22068632e-02 -2.29729474e-01 -3.42193455e-01\n",
      " -7.51401633e-02  1.34234384e-01  6.82203390e-04  1.77948594e-01\n",
      " -1.71288267e-01  1.62083343e-01 -9.21556633e-03 -2.82607432e-02\n",
      "  1.08047903e-01  1.78473502e-01 -2.01770663e-01 -9.03226882e-02\n",
      "  8.86956975e-02 -1.19083986e-01  1.02208197e-01  4.17280830e-02\n",
      " -2.73186266e-01 -9.47010070e-02  2.52874233e-02  2.45960765e-02\n",
      "  2.87340105e-01 -1.13201365e-01  7.40931556e-02 -2.94239521e-01\n",
      " -5.61320549e-03  4.76514339e-01 -6.76340312e-02 -1.08777590e-01\n",
      "  1.63192227e-01 -2.18740284e-01 -8.37993771e-02 -1.11572735e-01\n",
      " -1.20062687e-01 -2.61146396e-01 -3.95956300e-02  1.09679006e-01\n",
      " -3.73424701e-02  2.54462957e-02  1.40714675e-01  1.77820221e-01\n",
      "  8.17883089e-02 -2.17197821e-01 -1.51344925e-01  4.73140404e-02\n",
      " -1.84211642e-01 -1.18698955e-01  1.68079231e-02 -2.13847890e-01\n",
      "  2.18514040e-01  1.15026526e-01 -2.29200467e-01 -1.45268859e-02\n",
      "  2.47945204e-01 -3.87815893e-01 -1.22986116e-01  1.62654951e-01\n",
      " -1.21872433e-01  1.63069054e-01  3.24356109e-02  6.06061369e-02\n",
      "  3.89865637e-02  6.33745641e-02 -7.44284019e-02 -1.70707881e-01\n",
      " -6.68511167e-02 -8.58182684e-02  7.28557259e-02  1.05657749e-01\n",
      "  6.76974356e-02  3.98625731e-02  1.28728911e-01  3.75693738e-01\n",
      " -1.00366160e-01 -1.91284671e-01  1.58739835e-01  1.29238628e-02\n",
      " -4.79583023e-03 -1.37554333e-01  1.65259652e-03 -3.55842769e-01\n",
      " -4.72366102e-02 -1.09514920e-02  2.04005931e-02 -2.35657692e-01\n",
      " -3.43760438e-02 -1.50188237e-01  1.19084865e-01 -9.34914276e-02\n",
      "  3.07076931e-01  1.71448842e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "answer = word2vec.Word2Vec.load(\"word2vec1.model\")\n",
    "print(type(answer))\n",
    "\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "print(type(wvv), len(wvv))\n",
    "\n",
    "wvv_keys = wvv.keys()\n",
    "print(list(wvv.values())[:10])\n",
    "print(type(wvv_keys), len(wvv_keys))\n",
    "\n",
    "wvv_keys_list = list(wvv_keys)\n",
    "print(wvv_keys_list[:10])\n",
    "print(type(answer[\"覺得心\"]), len(answer[\"覺得心\"]),answer[\"覺得心\"])\n",
=======
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import word2vec\n",
    "\n",
    "# answer = word2vec.Word2Vec.load(\"word2vec1.model\")\n",
    "# print(type(answer))\n",
    "\n",
    "# word_vectors = answer.wv\n",
    "# wvv = word_vectors.vocab\n",
    "# print(type(wvv), len(wvv))\n",
    "\n",
    "# wvv_keys = wvv.keys()\n",
    "# print(list(wvv.values())[:10])\n",
    "# print(type(wvv_keys), len(wvv_keys))\n",
    "\n",
    "# wvv_keys_list = list(wvv_keys)\n",
    "# print(wvv_keys_list[:10])\n",
    "# print(type(answer[\"覺得心\"]), len(answer[\"覺得心\"]))\n",
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
    "\n",
    "# # wvv_new_dict = dict(wvv_keys_list)\n",
    "# wvv_new_dict = dict(enumerate(wvv_keys_list))\n",
    "# print(type(wvv_new_dict), len(wvv_new_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "ss = np.append( ss, [1, 2 ,3], axis=0)\n",
    "ss = np.append( ss, [ 3], axis=0)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature= np.asarray(test_embedding_matrix[10])\n",
    "# print(test_feature)\n",
    "predict_res = model.predict(test_feature, batch_size=5, verbose=0)\n",
    "print(len(predict_res), predict_res[0])\n",
    "\n",
    "final_res = []\n",
    "now_item = predict_res[0]\n",
    "max_val = max(now_item)\n",
    "final_res.append(np.argmax(now_item))\n",
    "print(final_res)\n",
    "\n",
    "# test_item = test_list[0]\n",
    "# test_feature= np.asarray([test_list[0]])\n",
    "# predict_res = model.predict(test_feature, batch_size=5, verbose=0)\n",
    "# final_res = []\n",
    "# print(len(test_list))\n",
    "# for test_item in test_list:\n",
    "#     test_feature= np.asarray([test_item])\n",
    "#     predict_res = model.predict(test_feature, batch_size=5, verbose=0)\n",
    "#     print(predict_res)\n",
    "# # for res in predict_res:\n",
    "#     now_item = res[0]\n",
    "#     max_val = max(now_item)\n",
    "#     final_res.append(np.argmax(now_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = word2vec.Word2Vec.load(\"word2vec1.model\")\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "wvv_keys = wvv.keys()\n",
    "wvv_keys_list = list(wvv_keys)\n",
    "wvv_new_dict1 = dict(enumerate(wvv_keys_list))\n",
    "print(type(wvv_new_dict1))\n",
    "test_embedding_matrix = np.zeros((len(test_dict) + 1, 400))\n",
    "# inss = 0\n",
    "for i,word in test_dict.items():\n",
    "#     print(i,train_dict[i][1], word, \"\\n\")\n",
    "#     if inss > 10:\n",
    "#         break\n",
    "    test_embedding_vector = wvv_new_dict1[word]\n",
    "    test_embedding_matrix[i] = test_embedding_vector\n",
    "# del answer\n",
    "# embeddings_index = list()\n",
    "# # word_vectors = answer.wv\n",
    "# MAX_NB_WORDS = len(word_index)\n",
    "# for word,vocab_obj in word_vectors.vocab.items():\n",
    "#     if int(vocab_obj.index) < MAX_NB_WORDS:\n",
    "#         embeddings_index[word] = word_vectors[word]\n",
    "# del answer, word_vectors\n",
    "# print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inss = 0\n",
    "for i,word in train_dict.items():\n",
    "#     print(i,train_dict[i][1], word, \"\\n\")\n",
    "#     if inss > 10:\n",
    "#         break\n",
    "    embedding_vector = getArticleVec(word)\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "#     inss += 1"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts data\n",
    "TRAINING_PATH = './data/training/'\n",
    "TESTING_PATH = './data/testing/'\n",
    "\n",
    "categories = [dirname for dirname in os.listdir(TRAINING_PATH) if dirname[-4:] != '_cut']\n",
    "# print(len(categories), str(categories))\n",
    "\n",
    "category2idx = {'Japan_Travel': 0, 'KR_ENTERTAIN': 1, 'Makeup': 2, 'Tech_Job':  3, 'WomenTalk': 4,\n",
    "                  'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "\n",
    "train_list = []\n",
    "for category in categories:\n",
    "    category_idx = category2idx[category]\n",
    "    category_path = TRAINING_PATH + category + '_cut/'\n",
    "    \n",
    "    for filename in os.listdir(category_path):\n",
    "        filepath = category_path + filename\n",
    "        \n",
    "        with open(filepath, encoding='utf-8') as file:\n",
    "            words = file.read().strip().split(' / ')\n",
    "            train_list.append([words, category_idx])\n",
    "            \n",
    "test_list = []\n",
    "category_path = TESTING_PATH \n",
    "    \n",
    "for filename in os.listdir(category_path):\n",
    "        filepath = category_path + filename\n",
    "        \n",
    "        with open(filepath, encoding='utf-8') as file:\n",
    "            words = file.read().strip().split(' / ')\n",
    "            test_list.append([words, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(word_index))\n",
    "# print(type(word_index), len(word_index))\n",
    "# ii = 0\n",
    "# for word,i in word_index.items():\n",
    "#     if ii > 10:\n",
    "#         break\n",
    "#     print(type(word), i, len(word), word)\n",
    "#     ii+=1\n",
    "# print(type(word_index[0]), word_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_len = len(embedding_matrix[0])\n",
    "label_list = np.zeros((embedding_matrix_len, 10))\n",
    "label_id = 0\n",
    "for i,article in train_dict.items():\n",
    "    article_len = len(article)\n",
    "    sec_i = article[1]\n",
    "    for w_i in range(0, article_len):\n",
    "        label_list[label_id][sec_i] = 1\n",
    "        label_id += 1\n",
    "#             break\n",
    "print(label_list[i])\n",
    "   \n",
    "# # process some data\n",
    "# train_texts_list = list(train_texts)\n",
    "train_labels_list = list(train_labels)\n",
    "\n",
    "# test_texts_list = list(test_texts)\n",
    "print(type(train_labels_list), len(train_labels_list), train_labels_list[0])\n",
    "# train_dict =  dict(enumerate(train_texts_list))\n",
    "# test_dict =  dict(enumerate(test_texts_list))\n",
    "\n",
    "# label_new_list= np.zeros((embedding_matrix_len, 10))\n",
    "# label_list_i = 0\n",
    "# for label in label_list:\n",
    "#     label_new = np.reshape(label_list[label_list_i],(1, 10))\n",
    "#     label_new_list[label_list_i] = label_new\n",
    "#     label_list_i += 1\n",
    "print(len(label_list))    \n",
    "# del label_list    \n",
    "# print(label_list.shape)\n",
    "# print(label_new_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "MAX_NB_WORDS = len(word_index)\n",
    "for word,vocab_obj in wvv.items():\n",
    "#     print(  word_vectors[word])\n",
    "    if int(vocab_obj.index) < MAX_NB_WORDS and word == -1:    \n",
    "        embeddings_index[word] = word_vectors[word]\n",
    "\n",
    "print(MAX_NB_WORDS)\n",
    "print(\"Found %s word vectors.\" % len(wvv))\n",
    "OUT_SIZE = 250\n",
    "\n",
    "def getArticleMatrix(article_temp):\n",
    "    res = np.zeros((3971,  OUT_SIZE))\n",
    "    i = 0\n",
    "    for word in article_temp:\n",
    "        word_embedding_vector = embeddings_index.get(word)\n",
    "        if word_embedding_vector is not None:\n",
    "            res[i] = word_embedding_vector\n",
    "            res[i]=np.reshape(res[i],(1, 250))\n",
    "    return res\n",
    "\n",
    "print(\"wvv : \", len(wvv))\n",
    "print(len(wvv[0]))\n",
    "# print(len(word_index[1]))\n",
    "\n",
    "print(len(train_dict))\n",
    "# print(train_dict[0][0])\n",
    "\n",
    "# ARTICLES_LEN = len(train_dict)\n",
    "# ARTICLES_LEN = 1000\n",
    "# articles_matrix = np.zeros((ARTICLES_LEN, 3971, 250))\n",
    "# for i,article in train_dict.items():\n",
    "#     if i < ARTICLES_LEN:\n",
    "#         temp_vec = getArticleMatrix(article[0])\n",
    "#         if temp_vec is not None:\n",
    "#             articles_matrix[i] = temp_vec\n",
    "#     else:\n",
    "#         break\n",
    "# print(len(articles_matrix))\n",
    "\n",
    "articles_num = len(train_dict)\n",
    "\n",
    "embedding_matrix = np.zeros( (len(word_index), 250))\n",
    "print(\"embedding_matrix\", embedding_matrix.shape)\n",
    "\n",
    "for i,word in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "#     print(embedding_vector.shape)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix_len = len(word_index)\n",
    "\n",
    "print(embedding_matrix_len, OUT_SIZE, 3971)\n",
    "embedding_layer = Embedding(embedding_matrix_len, 128,\n",
    "                            weights = [embeddings_index],\n",
    "                            input_length = 3971,\n",
    "                            trainable = False)\n",
    "print(\"articles_num\", articles_num)\n"
   ]
=======
>>>>>>> c11b4f33445e3dc7395f142955792c80d63dcb48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
