{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "# import keras\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN, GRU,Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from datetime import datetime\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts data\n",
    "category2idx = {'Japan_Travel': 0, 'KR_ENTERTAIN': 1, 'Makeup': 2, 'Tech_Job':  3, 'WomenTalk': 4,\n",
    "                  'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "\n",
    "train_df_sample = pd.read_pickle('train.pkl').sample(frac=1, random_state=123)\n",
    "# train_df_sample = pd.concat([train_pickle_df.text]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embedding vector\n",
    "answer = word2vec.Word2Vec.load(\"word2vec_20180425.model\")\n",
    "# print(type(answer))\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "wvv_keys = wvv.keys()\n",
    "wvv_keys_list = list(wvv_keys)\n",
    "# print(wvv_keys_list[:10]) #['櫻花林', '好比', '考科', '床上', '一點現', '記住', '寶寶的', '柔嫩', '不規則', '朴智妍']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df_sample.values\n",
    "label_list = to_categorical(train_df_sample.category)\n",
    "\n",
    "test_pickle_df = pd.read_pickle('test.pkl')\n",
    "test_texts = test_pickle_df[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word_index[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "#         new_doc_arr = np.array(new_doc).reshape(1, max_doc_word_length)\n",
    "        new_doc_arr = np.array(new_doc)\n",
    "        new_corpus.append( new_doc_arr)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_list = []\n",
    "for text in train_texts:\n",
    "    train_texts_list.append(text[0])\n",
    "\n",
    "# train_texts_index = train_texts_list\n",
    "\n",
    "# print(len(train_texts_list), train_texts_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = []\n",
    "for text in train_texts_list:\n",
    "    texts_list.append(text)\n",
    "    \n",
    "for text in test_texts:\n",
    "    texts_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 218242 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_list)\n",
    "sequences = tokenizer.texts_to_sequences(texts_list)\n",
    "# max_doc_word_length = max(len(l) for l in train_texts)\n",
    "max_doc_word_length = 200\n",
    "sequences1 = pad_sequences(sequences, maxlen=max_doc_word_length, padding='pre')\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "\n",
    "# data = pad_sequences(sequences)\n",
    "# print(\"Shape of data tensor:\" , data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1\n",
    "# create a weight matrix for words in training docs\n",
    "answer_vector_size = answer.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, answer_vector_size))\n",
    "for word, i in word_index.items():\n",
    "    if word in wvv_keys_list:\n",
    "        embedding_vector = answer[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i+1] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros((1, answer_vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 250)         54560750  \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               1700      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 54,586,376\n",
      "Trainable params: 25,626\n",
      "Non-trainable params: 54,560,750\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim= embedding_matrix.shape[0],\n",
    "                            output_dim= embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix], \n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(16))    \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=0, mode='auto')\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = text_to_index(train_df_sample.text)\n",
    "X_train = pad_sequences(X_train_texts, maxlen= max_doc_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 <class 'numpy.ndarray'>\n",
      "200 <class 'numpy.ndarray'>\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# X_train_texts_ = list(X_train_texts)\n",
    "print(len(X_train), type(X_train))\n",
    "print(len(X_train[0]), type(X_train[0]))\n",
    "print(X_train[0].shape)\n",
    "print(X_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 2.3011 - acc: 0.1160 - val_loss: 2.2923 - val_acc: 0.1222\n",
      "Epoch 2/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2874 - acc: 0.1315 - val_loss: 2.2815 - val_acc: 0.1422\n",
      "Epoch 3/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2746 - acc: 0.1548 - val_loss: 2.2704 - val_acc: 0.1489\n",
      "Epoch 4/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2609 - acc: 0.1638 - val_loss: 2.2595 - val_acc: 0.1611\n",
      "Epoch 5/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 2.2470 - acc: 0.1700 - val_loss: 2.2480 - val_acc: 0.1678\n",
      "Epoch 6/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 2.2334 - acc: 0.1767 - val_loss: 2.2378 - val_acc: 0.1800\n",
      "Epoch 7/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2210 - acc: 0.1793 - val_loss: 2.2286 - val_acc: 0.1778\n",
      "Epoch 8/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.2097 - acc: 0.1832 - val_loss: 2.2193 - val_acc: 0.1867\n",
      "Epoch 9/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.1987 - acc: 0.1880 - val_loss: 2.2103 - val_acc: 0.1933\n",
      "Epoch 10/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1873 - acc: 0.1968 - val_loss: 2.2009 - val_acc: 0.1978\n",
      "Epoch 11/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.1754 - acc: 0.2077 - val_loss: 2.1906 - val_acc: 0.2022\n",
      "Epoch 12/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.1629 - acc: 0.2165 - val_loss: 2.1807 - val_acc: 0.2178\n",
      "Epoch 13/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.1497 - acc: 0.2254 - val_loss: 2.1708 - val_acc: 0.2233\n",
      "Epoch 14/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1357 - acc: 0.2311 - val_loss: 2.1603 - val_acc: 0.2222\n",
      "Epoch 15/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 2.1214 - acc: 0.2374 - val_loss: 2.1504 - val_acc: 0.2178\n",
      "Epoch 16/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.1057 - acc: 0.2441 - val_loss: 2.1404 - val_acc: 0.2244\n",
      "Epoch 17/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.0901 - acc: 0.2537 - val_loss: 2.1301 - val_acc: 0.2278\n",
      "Epoch 18/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.0732 - acc: 0.2626 - val_loss: 2.1201 - val_acc: 0.2344\n",
      "Epoch 19/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.0550 - acc: 0.2702 - val_loss: 2.1106 - val_acc: 0.2300\n",
      "Epoch 20/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.0372 - acc: 0.2775 - val_loss: 2.0992 - val_acc: 0.2433\n",
      "Epoch 21/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 2.0171 - acc: 0.2848 - val_loss: 2.0879 - val_acc: 0.2411\n",
      "Epoch 22/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.9954 - acc: 0.2951 - val_loss: 2.0742 - val_acc: 0.2478\n",
      "Epoch 23/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.9719 - acc: 0.2996 - val_loss: 2.0564 - val_acc: 0.2633\n",
      "Epoch 24/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.9466 - acc: 0.3144 - val_loss: 2.0390 - val_acc: 0.2633\n",
      "Epoch 25/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.9226 - acc: 0.3241 - val_loss: 2.0151 - val_acc: 0.2667\n",
      "Epoch 26/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.8932 - acc: 0.3314 - val_loss: 1.9889 - val_acc: 0.2767\n",
      "Epoch 27/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.8659 - acc: 0.3412 - val_loss: 1.9639 - val_acc: 0.2800\n",
      "Epoch 28/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.8358 - acc: 0.3517 - val_loss: 1.9504 - val_acc: 0.2856\n",
      "Epoch 29/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.8288 - acc: 0.3535 - val_loss: 1.9163 - val_acc: 0.2922\n",
      "Epoch 30/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 1.7943 - acc: 0.3667 - val_loss: 1.9036 - val_acc: 0.2867\n",
      "Epoch 31/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.7715 - acc: 0.3741 - val_loss: 1.8813 - val_acc: 0.3000\n",
      "Epoch 32/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 1.7510 - acc: 0.3821 - val_loss: 1.8667 - val_acc: 0.3333\n",
      "Epoch 33/100\n",
      "8100/8100 [==============================] - 12s 2ms/step - loss: 1.7305 - acc: 0.3886 - val_loss: 1.8570 - val_acc: 0.3367\n",
      "Epoch 34/100\n",
      "8100/8100 [==============================] - 12s 2ms/step - loss: 1.7159 - acc: 0.3953 - val_loss: 1.8350 - val_acc: 0.3411\n",
      "Epoch 35/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.6920 - acc: 0.4006 - val_loss: 1.8246 - val_acc: 0.3533\n",
      "Epoch 36/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 1.6759 - acc: 0.4099 - val_loss: 1.8079 - val_acc: 0.3522\n",
      "Epoch 37/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.6559 - acc: 0.4165 - val_loss: 1.7888 - val_acc: 0.3600\n",
      "Epoch 38/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.6501 - acc: 0.4221 - val_loss: 1.7794 - val_acc: 0.3711\n",
      "Epoch 39/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.6369 - acc: 0.4270 - val_loss: 1.7661 - val_acc: 0.3722\n",
      "Epoch 40/100\n",
      "8100/8100 [==============================] - 12s 2ms/step - loss: 1.6201 - acc: 0.4254 - val_loss: 1.7341 - val_acc: 0.3800\n",
      "Epoch 41/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.6008 - acc: 0.4367 - val_loss: 1.7334 - val_acc: 0.3867\n",
      "Epoch 42/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.5845 - acc: 0.4437 - val_loss: 1.7179 - val_acc: 0.3978\n",
      "Epoch 43/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.5665 - acc: 0.4437 - val_loss: 1.6979 - val_acc: 0.3944\n",
      "Epoch 44/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.5532 - acc: 0.4491 - val_loss: 1.6868 - val_acc: 0.3989\n",
      "Epoch 45/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.5385 - acc: 0.4578 - val_loss: 1.6741 - val_acc: 0.4156\n",
      "Epoch 46/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.5241 - acc: 0.4621 - val_loss: 1.6624 - val_acc: 0.4156\n",
      "Epoch 47/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.5139 - acc: 0.4663 - val_loss: 1.6517 - val_acc: 0.4233\n",
      "Epoch 48/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.5035 - acc: 0.4694 - val_loss: 1.6477 - val_acc: 0.4233\n",
      "Epoch 49/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.4917 - acc: 0.4753 - val_loss: 1.6340 - val_acc: 0.4367\n",
      "Epoch 50/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.4787 - acc: 0.4804 - val_loss: 1.6219 - val_acc: 0.4300\n",
      "Epoch 51/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.4688 - acc: 0.4822 - val_loss: 1.6347 - val_acc: 0.4300\n",
      "Epoch 52/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.4766 - acc: 0.4793 - val_loss: 1.6261 - val_acc: 0.4267\n",
      "Epoch 53/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.4602 - acc: 0.4857 - val_loss: 1.6099 - val_acc: 0.4356\n",
      "Epoch 54/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.4466 - acc: 0.4910 - val_loss: 1.6009 - val_acc: 0.4422\n",
      "Epoch 55/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.4403 - acc: 0.4931 - val_loss: 1.5793 - val_acc: 0.4478\n",
      "Epoch 56/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.4253 - acc: 0.5004 - val_loss: 1.5720 - val_acc: 0.4544\n",
      "Epoch 57/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.4202 - acc: 0.5030 - val_loss: 1.5628 - val_acc: 0.4611\n",
      "Epoch 58/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.4064 - acc: 0.5062 - val_loss: 1.5576 - val_acc: 0.4578\n",
      "Epoch 59/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.3970 - acc: 0.5084 - val_loss: 1.5461 - val_acc: 0.4633\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.3863 - acc: 0.5117 - val_loss: 1.5388 - val_acc: 0.4644\n",
      "Epoch 61/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.3757 - acc: 0.5177 - val_loss: 1.5313 - val_acc: 0.4700\n",
      "Epoch 62/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 1.3709 - acc: 0.5211 - val_loss: 1.5248 - val_acc: 0.4678\n",
      "Epoch 63/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.3637 - acc: 0.5212 - val_loss: 1.5172 - val_acc: 0.4767\n",
      "Epoch 64/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.3540 - acc: 0.5253 - val_loss: 1.5058 - val_acc: 0.4789\n",
      "Epoch 65/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.3426 - acc: 0.5294 - val_loss: 1.5000 - val_acc: 0.4867\n",
      "Epoch 66/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.3339 - acc: 0.5341 - val_loss: 1.4957 - val_acc: 0.4833\n",
      "Epoch 67/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.3226 - acc: 0.5368 - val_loss: 1.4899 - val_acc: 0.4844\n",
      "Epoch 68/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.3168 - acc: 0.5430 - val_loss: 1.4808 - val_acc: 0.4867\n",
      "Epoch 69/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.3098 - acc: 0.5416 - val_loss: 1.4760 - val_acc: 0.4956\n",
      "Epoch 70/100\n",
      "8100/8100 [==============================] - 12s 2ms/step - loss: 1.2995 - acc: 0.5472 - val_loss: 1.4709 - val_acc: 0.4911\n",
      "Epoch 71/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2923 - acc: 0.5509 - val_loss: 1.4633 - val_acc: 0.5000\n",
      "Epoch 72/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2901 - acc: 0.5486 - val_loss: 1.4657 - val_acc: 0.5033\n",
      "Epoch 73/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.2806 - acc: 0.5549 - val_loss: 1.4517 - val_acc: 0.5056\n",
      "Epoch 74/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.2747 - acc: 0.5531 - val_loss: 1.4365 - val_acc: 0.5167\n",
      "Epoch 75/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.2642 - acc: 0.5581 - val_loss: 1.4305 - val_acc: 0.5256\n",
      "Epoch 76/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2539 - acc: 0.5617 - val_loss: 1.4313 - val_acc: 0.5156\n",
      "Epoch 77/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2493 - acc: 0.5626 - val_loss: 1.4244 - val_acc: 0.5156\n",
      "Epoch 78/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2401 - acc: 0.5688 - val_loss: 1.4173 - val_acc: 0.5167\n",
      "Epoch 79/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2313 - acc: 0.5694 - val_loss: 1.4104 - val_acc: 0.5211\n",
      "Epoch 80/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2261 - acc: 0.5728 - val_loss: 1.4020 - val_acc: 0.5233\n",
      "Epoch 81/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2143 - acc: 0.5764 - val_loss: 1.4027 - val_acc: 0.5244\n",
      "Epoch 82/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.2095 - acc: 0.5786 - val_loss: 1.4115 - val_acc: 0.5278\n",
      "Epoch 83/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.2116 - acc: 0.5794 - val_loss: 1.3861 - val_acc: 0.5411\n",
      "Epoch 84/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1951 - acc: 0.5847 - val_loss: 1.3780 - val_acc: 0.5389\n",
      "Epoch 85/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1867 - acc: 0.5868 - val_loss: 1.3744 - val_acc: 0.5422\n",
      "Epoch 86/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1919 - acc: 0.5851 - val_loss: 1.3649 - val_acc: 0.5522\n",
      "Epoch 87/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1822 - acc: 0.5909 - val_loss: 1.3611 - val_acc: 0.5500\n",
      "Epoch 88/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1703 - acc: 0.5973 - val_loss: 1.3578 - val_acc: 0.5533\n",
      "Epoch 89/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1660 - acc: 0.5933 - val_loss: 1.3595 - val_acc: 0.5411\n",
      "Epoch 90/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1640 - acc: 0.5964 - val_loss: 1.3448 - val_acc: 0.5522\n",
      "Epoch 91/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1514 - acc: 0.6021 - val_loss: 1.3668 - val_acc: 0.5356\n",
      "Epoch 92/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1724 - acc: 0.5902 - val_loss: 1.3445 - val_acc: 0.5467\n",
      "Epoch 93/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1524 - acc: 0.6014 - val_loss: 1.3461 - val_acc: 0.5389\n",
      "Epoch 94/100\n",
      "6000/8100 [=====================>........] - ETA: 2s - loss: 1.1452 - acc: 0.6058"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = label_list, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size= 3000,\n",
    "                    epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func_name = 'categorical_crossentropy'\n",
    "# for value in history.history[\"categorical_crossentropy\"]:\n",
    "#     print(history)\n",
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss_accuracy = model.evaluate(sequences1[0:200], label_list[0:200], verbose=1)\n",
    "print(type(loss_accuracy), loss_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tokenizer = Tokenizer()\n",
    "# test_tokenizer.fit_on_texts(test_texts)\n",
    "# test_sequences = test_tokenizer.texts_to_sequences(test_texts)\n",
    "# test_sequences1 = pad_sequences(test_sequences, maxlen=max_doc_word_length, padding='post')\n",
    "\n",
    "Y_sequences1 = text_to_index(test_pickle_df.text)\n",
    "Y_sequences11 = pad_sequences(Y_sequences1, maxlen=max_doc_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_res = model.predict(Y_sequences11, verbose=1)\n",
    "# print(len(predict_res), predict_res)\n",
    "\n",
    "final_res = []\n",
    "for pre_res in predict_res:\n",
    "    final_res.append(np.argmax(pre_res))\n",
    "\n",
    "    \n",
    "# 'Japan_Travel': 0, 'KR_ENTERTAIN' 娱乐: 1, 'Makeup' 化妆 : 2, 'Tech_Job':  3, 'WomenTalk': 4, \n",
    "# 'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "# 7, 9, 4, 5, 1/2 ,  1/8, 3/7, 1, 7, 0\n",
    "print(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "print(len(final_res))\n",
    "result_txt = \"result001\" + \".txt\"\n",
    "ids = 0\n",
    "with open(result_txt, 'w') as out:\n",
    "    out.write(\"id,category\" + '\\n')\n",
    "    for value in final_res:\n",
    "        out.write(str(ids) + \",\" + str(value) + '\\n')\n",
    "        ids += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
