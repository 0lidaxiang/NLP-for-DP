{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: UTF-8 -*-\n",
    "import time\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "# import keras\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN, GRU,Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from datetime import datetime\n",
    "from gensim.models import word2vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts data\n",
    "category2idx = {'Japan_Travel': 0, 'KR_ENTERTAIN': 1, 'Makeup': 2, 'Tech_Job':  3, 'WomenTalk': 4,\n",
    "                  'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "\n",
    "train_df_sample = pd.read_pickle('train.pkl').sample(frac=1, random_state=123)\n",
    "# train_df_sample = pd.concat([train_pickle_df.text]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embedding vector\n",
    "answer = word2vec.Word2Vec.load(\"word2vec_20180425.model\")\n",
    "# print(type(answer))\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "wvv_keys = wvv.keys()\n",
    "wvv_keys_list = list(wvv_keys)\n",
    "# print(wvv_keys_list[:10]) #['櫻花林', '好比', '考科', '床上', '一點現', '記住', '寶寶的', '柔嫩', '不規則', '朴智妍']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df_sample.values\n",
    "label_list = to_categorical(train_df_sample.category)\n",
    "\n",
    "test_pickle_df = pd.read_pickle('test.pkl')\n",
    "test_texts = test_pickle_df[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_list = []\n",
    "for text in train_texts:\n",
    "    train_texts_list.append(text[0])\n",
    "\n",
    "# train_texts_index = train_texts_list\n",
    "\n",
    "# print(len(train_texts_list), train_texts_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = []\n",
    "for text in train_texts_list:\n",
    "    texts_list.append(text)\n",
    "    \n",
    "for text in test_texts:\n",
    "    texts_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 218242 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts_list)\n",
    "sequences = tokenizer.texts_to_sequences(texts_list)\n",
    "# max_doc_word_length = max(len(l) for l in train_texts)\n",
    "max_doc_word_length = 200\n",
    "sequences1 = pad_sequences(sequences, maxlen=max_doc_word_length, padding='pre')\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Found %s unique tokens\" % len(word_index))\n",
    "\n",
    "\n",
    "del texts_list,train_texts_list,test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'> 218242\n",
      "<class 'list'> 218242\n",
      "218242\n",
      "34453\n",
      "41796\n"
     ]
    }
   ],
   "source": [
    "word_counts = tokenizer.word_counts\n",
    "print(type(word_counts), len(word_counts))\n",
    "new_word_counts = list(word_counts.items())\n",
    "print(type(new_word_counts), len(new_word_counts))\n",
    "new_word_counts = sorted(new_word_counts, key=lambda s: s[1], reverse=True)\n",
    "ii = 0\n",
    "print(len(word_index))\n",
    "# new_word_counts[20:41142]\n",
    "for word in new_word_counts:\n",
    "    if word[1] < 4:\n",
    "        del word_index[word[0]]\n",
    "    if word[1] > 4000:\n",
    "#         print(word[0], word[1])\n",
    "        del word_index[word[0]]\n",
    "print(len(wvv_keys_list))\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others_words = []\n",
    "# for k,w in word_index.items():\n",
    "#     if w >41797:\n",
    "# #         del word_index[k]\n",
    "#         others_words.append(k)\n",
    "#         print(k,w)\n",
    "# for w in others_words:\n",
    "#     del word_index[w]\n",
    "# print(\"-------------\") \n",
    "\n",
    "word_index_list= []\n",
    "for key, value in word_index.items():\n",
    "    temp = [key,value]\n",
    "    word_index_list.append(key)\n",
    "del word_index\n",
    "new_word_index = {word_index_list[i]: i for i in range(0, len(word_index_list))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41796\n",
      "['張', '戴隱形', '寫實', '聊得', '北美票房']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(new_word_index))\n",
    "print(word_index_list[:5])\n",
    "print(new_word_index['戴隱形'])\n",
    "# for k,w in word_index.items():\n",
    "# #     if w >41797:\n",
    "# # #         del word_index[k]\n",
    "# #         others_words.append(k)\n",
    "#         print(k,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41797\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(new_word_index) + 1\n",
    "# create a weight matrix for words in training docs\n",
    "answer_vector_size = answer.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, answer_vector_size))\n",
    "embed_i = 1\n",
    "for word in new_word_index:\n",
    "    if word in wvv_keys_list:\n",
    "        embedding_vector = answer[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[embed_i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[embed_i] = np.zeros((1, answer_vector_size))\n",
    "    embed_i += 1\n",
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "del answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 250)         10449250  \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 16)                12816     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               1700      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 10,474,876\n",
      "Trainable params: 25,626\n",
      "Non-trainable params: 10,449,250\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim= embedding_matrix.shape[0],\n",
    "                            output_dim= embedding_matrix.shape[1],\n",
    "                            weights=[embedding_matrix], \n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(16))    \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=0, mode='auto')\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',  metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(new_word_index[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "#         new_doc_arr = np.array(new_doc).reshape(1, max_doc_word_length)\n",
    "        new_doc_arr = np.array(new_doc)\n",
    "        new_corpus.append( new_doc_arr)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_texts = text_to_index(train_df_sample.text)\n",
    "X_train = pad_sequences(X_train_texts, maxlen= max_doc_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 <class 'numpy.ndarray'>\n",
      "200 <class 'numpy.ndarray'>\n",
      "(200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# X_train_texts_ = list(X_train_texts)\n",
    "print(len(X_train), type(X_train))\n",
    "print(len(X_train[0]), type(X_train[0]))\n",
    "print(X_train[0].shape)\n",
    "print(X_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1133 - acc: 0.6101 - val_loss: 1.8956 - val_acc: 0.4156\n",
      "Epoch 2/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1221 - acc: 0.6041 - val_loss: 1.9104 - val_acc: 0.4122\n",
      "Epoch 3/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1122 - acc: 0.6141 - val_loss: 1.9040 - val_acc: 0.4056\n",
      "Epoch 4/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1070 - acc: 0.6121 - val_loss: 1.8971 - val_acc: 0.4167\n",
      "Epoch 5/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1075 - acc: 0.6143 - val_loss: 1.9118 - val_acc: 0.4022\n",
      "Epoch 6/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1120 - acc: 0.6093 - val_loss: 1.8952 - val_acc: 0.4056\n",
      "Epoch 7/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.1013 - acc: 0.6154 - val_loss: 1.8998 - val_acc: 0.4067\n",
      "Epoch 8/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0986 - acc: 0.6162 - val_loss: 1.9004 - val_acc: 0.4100\n",
      "Epoch 9/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0985 - acc: 0.6159 - val_loss: 1.9005 - val_acc: 0.4067\n",
      "Epoch 10/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0987 - acc: 0.6164 - val_loss: 1.9049 - val_acc: 0.4044\n",
      "Epoch 11/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0975 - acc: 0.6162 - val_loss: 1.9002 - val_acc: 0.4078\n",
      "Epoch 12/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0981 - acc: 0.6172 - val_loss: 1.8997 - val_acc: 0.4067\n",
      "Epoch 13/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0968 - acc: 0.6163 - val_loss: 1.9081 - val_acc: 0.4089\n",
      "Epoch 14/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0971 - acc: 0.6158 - val_loss: 1.9003 - val_acc: 0.4078\n",
      "Epoch 15/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0985 - acc: 0.6172 - val_loss: 1.9000 - val_acc: 0.4033\n",
      "Epoch 16/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0957 - acc: 0.6169 - val_loss: 1.9031 - val_acc: 0.4000\n",
      "Epoch 17/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0961 - acc: 0.6175 - val_loss: 1.9003 - val_acc: 0.4167\n",
      "Epoch 18/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1020 - acc: 0.6143 - val_loss: 1.9036 - val_acc: 0.4033\n",
      "Epoch 19/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0969 - acc: 0.6173 - val_loss: 1.9040 - val_acc: 0.4044\n",
      "Epoch 20/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0951 - acc: 0.6172 - val_loss: 1.9009 - val_acc: 0.4078\n",
      "Epoch 21/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0961 - acc: 0.6181 - val_loss: 1.9072 - val_acc: 0.4044\n",
      "Epoch 22/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1014 - acc: 0.6159 - val_loss: 1.9097 - val_acc: 0.4033\n",
      "Epoch 23/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0958 - acc: 0.6185 - val_loss: 1.9046 - val_acc: 0.4167\n",
      "Epoch 24/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0999 - acc: 0.6141 - val_loss: 1.9128 - val_acc: 0.4056\n",
      "Epoch 25/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0937 - acc: 0.6188 - val_loss: 1.9039 - val_acc: 0.4067\n",
      "Epoch 26/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0995 - acc: 0.6138 - val_loss: 1.9066 - val_acc: 0.4067\n",
      "Epoch 27/100\n",
      "8100/8100 [==============================] - 12s 2ms/step - loss: 1.0940 - acc: 0.6186 - val_loss: 1.9138 - val_acc: 0.4044\n",
      "Epoch 28/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.1006 - acc: 0.6123 - val_loss: 1.9127 - val_acc: 0.4044\n",
      "Epoch 29/100\n",
      "8100/8100 [==============================] - 12s 2ms/step - loss: 1.0943 - acc: 0.6184 - val_loss: 1.9047 - val_acc: 0.4078\n",
      "Epoch 30/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0908 - acc: 0.6200 - val_loss: 1.9058 - val_acc: 0.4033\n",
      "Epoch 31/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0903 - acc: 0.6201 - val_loss: 1.9049 - val_acc: 0.4078\n",
      "Epoch 32/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0913 - acc: 0.6202 - val_loss: 1.9072 - val_acc: 0.4067\n",
      "Epoch 33/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0908 - acc: 0.6199 - val_loss: 1.9098 - val_acc: 0.4033\n",
      "Epoch 34/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0901 - acc: 0.6200 - val_loss: 1.9051 - val_acc: 0.4089\n",
      "Epoch 35/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0940 - acc: 0.6185 - val_loss: 1.9061 - val_acc: 0.4100\n",
      "Epoch 36/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0905 - acc: 0.6186 - val_loss: 1.9192 - val_acc: 0.4056\n",
      "Epoch 37/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0980 - acc: 0.6164 - val_loss: 1.9103 - val_acc: 0.4067\n",
      "Epoch 38/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0924 - acc: 0.6195 - val_loss: 1.9112 - val_acc: 0.4156\n",
      "Epoch 39/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0989 - acc: 0.6165 - val_loss: 1.9191 - val_acc: 0.4056\n",
      "Epoch 40/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0991 - acc: 0.6162 - val_loss: 1.9116 - val_acc: 0.4078\n",
      "Epoch 41/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0914 - acc: 0.6191 - val_loss: 1.9096 - val_acc: 0.4144\n",
      "Epoch 42/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0968 - acc: 0.6179 - val_loss: 1.9113 - val_acc: 0.4089\n",
      "Epoch 43/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0909 - acc: 0.6215 - val_loss: 1.9207 - val_acc: 0.4078\n",
      "Epoch 44/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0910 - acc: 0.6193 - val_loss: 1.9093 - val_acc: 0.4056\n",
      "Epoch 45/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0877 - acc: 0.6230 - val_loss: 1.9113 - val_acc: 0.4067\n",
      "Epoch 46/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0869 - acc: 0.6215 - val_loss: 1.9147 - val_acc: 0.4056\n",
      "Epoch 47/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0899 - acc: 0.6190 - val_loss: 1.9184 - val_acc: 0.4067\n",
      "Epoch 48/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0884 - acc: 0.6198 - val_loss: 1.9125 - val_acc: 0.4067\n",
      "Epoch 49/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0874 - acc: 0.6214 - val_loss: 1.9121 - val_acc: 0.4078\n",
      "Epoch 50/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0881 - acc: 0.6191 - val_loss: 1.9181 - val_acc: 0.4056\n",
      "Epoch 51/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0877 - acc: 0.6212 - val_loss: 1.9124 - val_acc: 0.4056\n",
      "Epoch 52/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0890 - acc: 0.6196 - val_loss: 1.9122 - val_acc: 0.4156\n",
      "Epoch 53/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0944 - acc: 0.6198 - val_loss: 1.9163 - val_acc: 0.4044\n",
      "Epoch 54/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0927 - acc: 0.6215 - val_loss: 1.9268 - val_acc: 0.4056\n",
      "Epoch 55/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0890 - acc: 0.6194 - val_loss: 1.9102 - val_acc: 0.4100\n",
      "Epoch 56/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0936 - acc: 0.6160 - val_loss: 1.9135 - val_acc: 0.4067\n",
      "Epoch 57/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0932 - acc: 0.6186 - val_loss: 1.9446 - val_acc: 0.4056\n",
      "Epoch 58/100\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 1.0993 - acc: 0.6107 - val_loss: 1.9160 - val_acc: 0.4133\n",
      "Epoch 59/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.1014 - acc: 0.6112 - val_loss: 1.9141 - val_acc: 0.4100\n",
      "Epoch 60/100\n",
      "8100/8100 [==============================] - 12s 2ms/step - loss: 1.0946 - acc: 0.6174 - val_loss: 1.9423 - val_acc: 0.4089\n",
      "Epoch 61/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0955 - acc: 0.6177 - val_loss: 1.9152 - val_acc: 0.4178\n",
      "Epoch 62/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.1057 - acc: 0.6125 - val_loss: 1.9105 - val_acc: 0.4067\n",
      "Epoch 63/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0930 - acc: 0.6165 - val_loss: 1.9362 - val_acc: 0.4078\n",
      "Epoch 64/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0908 - acc: 0.6174 - val_loss: 1.9122 - val_acc: 0.4167\n",
      "Epoch 65/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0941 - acc: 0.6186 - val_loss: 1.9199 - val_acc: 0.4089\n",
      "Epoch 66/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0895 - acc: 0.6194 - val_loss: 1.9226 - val_acc: 0.4078\n",
      "Epoch 67/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0855 - acc: 0.6198 - val_loss: 1.9155 - val_acc: 0.4078\n",
      "Epoch 68/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0868 - acc: 0.6211 - val_loss: 1.9149 - val_acc: 0.4044\n",
      "Epoch 69/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0817 - acc: 0.6216 - val_loss: 1.9181 - val_acc: 0.4067\n",
      "Epoch 70/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0820 - acc: 0.6232 - val_loss: 1.9236 - val_acc: 0.4100\n",
      "Epoch 71/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0852 - acc: 0.6233 - val_loss: 1.9181 - val_acc: 0.4078\n",
      "Epoch 72/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0834 - acc: 0.6226 - val_loss: 1.9137 - val_acc: 0.4122\n",
      "Epoch 73/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0847 - acc: 0.6230 - val_loss: 1.9234 - val_acc: 0.4078\n",
      "Epoch 74/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0842 - acc: 0.6246 - val_loss: 1.9172 - val_acc: 0.4056\n",
      "Epoch 75/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0805 - acc: 0.6214 - val_loss: 1.9191 - val_acc: 0.4100\n",
      "Epoch 76/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0802 - acc: 0.6241 - val_loss: 1.9293 - val_acc: 0.4089\n",
      "Epoch 77/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0881 - acc: 0.6191 - val_loss: 1.9238 - val_acc: 0.4067\n",
      "Epoch 78/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0808 - acc: 0.6225 - val_loss: 1.9143 - val_acc: 0.4067\n",
      "Epoch 79/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0834 - acc: 0.6253 - val_loss: 1.9215 - val_acc: 0.4067\n",
      "Epoch 80/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0805 - acc: 0.6248 - val_loss: 1.9230 - val_acc: 0.4111\n",
      "Epoch 81/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0816 - acc: 0.6223 - val_loss: 1.9269 - val_acc: 0.4100\n",
      "Epoch 82/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0800 - acc: 0.6233 - val_loss: 1.9195 - val_acc: 0.4067\n",
      "Epoch 83/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0811 - acc: 0.6219 - val_loss: 1.9225 - val_acc: 0.4033\n",
      "Epoch 84/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0782 - acc: 0.6252 - val_loss: 1.9284 - val_acc: 0.4056\n",
      "Epoch 85/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0876 - acc: 0.6193 - val_loss: 1.9328 - val_acc: 0.4122\n",
      "Epoch 86/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0794 - acc: 0.6235 - val_loss: 1.9196 - val_acc: 0.4144\n",
      "Epoch 87/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0899 - acc: 0.6204 - val_loss: 1.9227 - val_acc: 0.4111\n",
      "Epoch 88/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0797 - acc: 0.6221 - val_loss: 1.9270 - val_acc: 0.4078\n",
      "Epoch 89/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0794 - acc: 0.6217 - val_loss: 1.9244 - val_acc: 0.4111\n",
      "Epoch 90/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0779 - acc: 0.6241 - val_loss: 1.9184 - val_acc: 0.4111\n",
      "Epoch 91/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0800 - acc: 0.6236 - val_loss: 1.9306 - val_acc: 0.4111\n",
      "Epoch 92/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0835 - acc: 0.6188 - val_loss: 1.9338 - val_acc: 0.4056\n",
      "Epoch 93/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0799 - acc: 0.6214 - val_loss: 1.9276 - val_acc: 0.4133\n",
      "Epoch 94/100\n",
      "8100/8100 [==============================] - 12s 1ms/step - loss: 1.0907 - acc: 0.6195 - val_loss: 1.9238 - val_acc: 0.4100\n",
      "Epoch 95/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0803 - acc: 0.6215 - val_loss: 1.9476 - val_acc: 0.4078\n",
      "Epoch 96/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0832 - acc: 0.6221 - val_loss: 1.9214 - val_acc: 0.4056\n",
      "Epoch 97/100\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.0795 - acc: 0.6236 - val_loss: 1.9231 - val_acc: 0.4100\n",
      "Epoch 98/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0775 - acc: 0.6215 - val_loss: 1.9271 - val_acc: 0.4078\n",
      "Epoch 99/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0781 - acc: 0.6227 - val_loss: 1.9339 - val_acc: 0.4111\n",
      "Epoch 100/100\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 1.0755 - acc: 0.6252 - val_loss: 1.9225 - val_acc: 0.4111\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = X_train, y = label_list, \n",
    "                    validation_split=0.1, \n",
    "                    batch_size= 4050,\n",
    "                    epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func_name = 'categorical_crossentropy'\n",
    "# for value in history.history[\"categorical_crossentropy\"]:\n",
    "#     print(history)\n",
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate the model\n",
    "# loss_accuracy = model.evaluate(sequences1[0:200], label_list[0:200], verbose=1)\n",
    "# print(type(loss_accuracy), loss_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 11100 32721  3549  7078 13678 34042 37928 17456  4631 22489 19390     0\n",
      " 26520     0   953 25497     0  5859 15885     0 19828 25497   559     0\n",
      " 33028 13253 22746 27070 29849 30184  9925  7329 23712  5695  7747  6630\n",
      " 13253 22490     0 10247     0 41566     0 33028 21683  9817  9572     0\n",
      " 16997 26563 41219     0 12543 13625 27070 29849 10699 14817 23359 33284\n",
      " 41219 25565 11303 27070 38683 20474 24195 38854 40905 35139 34936 29098\n",
      " 18360 12032 36316 37300  5610 34936     0 34259 16561     0 35083 10247\n",
      " 13070  5666 32104 41219 33028 13253 38336 41219     0     0 13353 33115\n",
      " 40895 29762 21275 29762  7698 13267 33028 13253 38336     0 32104 10247\n",
      " 16721  7190  4722 23570     0 11676 32551  2851  4722 24195 13150     0\n",
      "  4112 19390  9373 33922 25816  5666  5934   471 10247     0 30729 37928\n",
      " 17456 33096 12497 19708 18287 27070 21390 11244 22290 39026 14370 31826\n",
      " 32149     0     0 12579 20226 32702     0 23351 19484  9925 31156 29318\n",
      " 23807 31786     0     0  2657 22264 38858 38144     0 35601  7655 23399\n",
      " 41782  9536 22490 32489 41263 36661 15751 35586     0     0 13166 12278\n",
      "     0 36582 28717  4782  8374 40714 26934 38402]\n"
     ]
    }
   ],
   "source": [
    "# test_tokenizer = Tokenizer()\n",
    "# test_tokenizer.fit_on_texts(test_texts)\n",
    "# test_sequences = test_tokenizer.texts_to_sequences(test_texts)\n",
    "# test_sequences1 = pad_sequences(test_sequences, maxlen=max_doc_word_length, padding='post')\n",
    "# X_train_texts = text_to_index(train_df_sample.text)\n",
    "# X_train = pad_sequences(X_train_texts, maxlen= max_doc_word_length)\n",
    "\n",
    "Y_sequences1 = text_to_index(test_pickle_df.text)\n",
    "\n",
    "Y_test = pad_sequences(Y_sequences1, maxlen=max_doc_word_length)\n",
    "print(Y_test[732])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "[0, 9, 2, 0, 8, 8, 5, 7, 3, 6, 0, 4, 2, 8, 4, 9, 9, 3, 1, 9, 3, 7, 9, 2, 1, 4, 8, 4, 8, 5, 0, 0, 0, 0, 0, 6, 0, 4, 1, 8, 4, 4, 3, 5, 8, 9, 6, 0, 6, 7, 0, 5, 7, 2, 5, 4, 5, 9, 2, 0, 6, 6, 0, 3, 7, 4, 8, 5, 0, 3, 0, 0, 7, 7, 8, 6, 7, 4, 3, 2, 2, 9, 1, 1, 5, 2, 2, 7, 4, 0, 6, 3, 4, 0, 9, 3, 6, 2, 4, 4, 7, 8, 2, 9, 7, 4, 7, 2, 2, 3, 8, 7, 5, 8, 4, 0, 9, 7, 4, 9, 8, 9, 5, 6, 3, 8, 8, 1, 6, 7, 7, 6, 6, 3, 7, 4, 9, 4, 1, 0, 4, 7, 3, 7, 2, 6, 2, 3, 0, 6, 7, 3, 9, 6, 7, 2, 6, 5, 2, 9, 6, 6, 8, 6, 2, 7, 8, 5, 2, 7, 7, 9, 8, 4, 7, 8, 2, 2, 4, 4, 1, 4, 1, 2, 2, 7, 2, 3, 0, 0, 3, 4, 8, 0, 8, 3, 8, 8, 6, 8, 2, 7, 3, 6, 3, 3, 8, 6, 6, 9, 4, 2, 2, 2, 3, 0, 2, 3, 2, 4, 0, 4, 9, 7, 9, 4, 1, 6, 4, 9, 4, 1, 0, 4, 7, 6, 0, 8, 3, 8, 9, 5, 6, 6, 2, 2, 2, 7, 8, 3, 9, 0, 0, 4, 9, 6, 4, 6, 4, 1, 1, 8, 1, 6, 4, 2, 6, 0, 7, 3, 7, 6, 2, 8, 6, 1, 7, 9, 9, 0, 7, 4, 4, 7, 8, 0, 4, 5, 2, 2, 5, 0, 0, 8, 6, 3, 5, 4, 7, 9, 6, 8, 0, 6, 0, 6, 5, 9, 3, 9, 2, 8, 2, 8, 8, 0, 3, 2, 8, 8, 3, 9, 2, 0, 0, 2, 2, 4, 7, 7, 5, 6, 3, 2, 0, 4, 3, 8, 4, 9, 9, 7, 6, 5, 8, 8, 0, 0, 8, 9, 0, 1, 3, 0, 6, 7, 5, 7, 9, 3, 2, 0, 8, 4, 3, 8, 9, 7, 4, 9, 8, 2, 0, 9, 2, 7, 4, 1, 0, 7, 8, 6, 6, 8, 1, 4, 6, 6, 6, 0, 6, 9, 5, 2, 5, 7, 4, 9, 9, 7, 0, 7, 8, 7, 3, 5, 8, 4, 3, 9, 1, 9, 1, 3, 8, 2, 0, 1, 7, 0, 8, 9, 0, 5, 8, 2, 2, 8, 0, 3, 9, 9, 8, 0, 1, 6, 2, 2, 7, 1, 2, 6, 2, 0, 9, 1, 8, 9, 8, 5, 6, 7, 9, 0, 8, 0, 9, 7, 9, 8, 3, 6, 1, 6, 0, 0, 3, 2, 3, 1, 8, 3, 2, 9, 7, 6, 7, 7, 0, 8, 0, 8, 5, 7, 3, 6, 9, 9, 3, 4, 3, 8, 7, 9, 7, 6, 1, 9, 4, 1, 0, 3, 2, 8, 6, 4, 5, 1, 0, 5, 5, 9, 8, 8, 6, 2, 0, 0, 6, 9, 4, 4, 4, 4, 7, 7, 9, 8, 5, 3, 4, 6, 3, 6, 8, 6, 1, 6, 7, 1, 1, 4, 0, 4, 5, 7, 0, 3, 4, 0, 6, 5, 8, 5, 3, 8, 0, 6, 1, 0, 7, 8, 7, 8, 9, 1, 6, 2, 2, 3, 2, 3, 0, 0, 8, 6, 2, 9, 3, 8, 1, 9, 2, 6, 8, 6, 3, 1, 4, 4, 0, 9, 2, 3, 3, 8, 7, 1, 4, 4, 4, 6, 6, 4, 1, 3, 0, 6, 8, 5, 9, 2, 0, 3, 7, 0, 3, 3, 1, 9, 6, 8, 2, 3, 8, 8, 3, 2, 9, 5, 4, 0, 9, 8, 0, 5, 9, 5, 9, 8, 8, 9, 5, 8, 7, 7, 2, 2, 8, 7, 1, 4, 5, 3, 0, 7, 4, 7, 6, 0, 0, 7, 0, 9, 3, 9, 6, 1, 3, 3, 7, 9, 4, 0, 6, 1, 7, 7, 6, 6, 4, 3, 4, 8, 3, 4, 7, 0, 6, 9, 9, 2, 6, 6, 1, 8, 0, 4, 9, 7, 0, 8, 6, 6, 2, 6, 8, 1, 5, 8, 4, 8, 0, 6, 5, 6, 8, 0, 8, 3, 1, 5, 8, 8, 8, 9, 8, 0, 9, 6, 4, 6, 2, 4, 6, 8, 9, 9, 7, 2, 4, 9, 8, 4, 9, 0, 0, 8, 9, 2, 9, 3, 4, 4, 2, 1, 4, 0, 9, 0, 0, 6, 3, 7, 2, 4, 3, 1, 9, 3, 3, 0, 0, 9, 3, 0, 1, 9, 9, 3, 0, 0, 2, 5, 8, 3, 1, 8, 6, 8, 4, 6, 4, 4, 3, 4, 5, 2, 2, 1, 3, 2, 1, 8, 0, 0, 7, 6, 2, 4, 5, 3, 5, 8, 6, 8, 9, 9, 7, 2, 2, 2, 2, 4, 7, 1, 4, 4, 4, 8, 4, 9, 4, 5, 1, 6, 3, 2, 2, 6, 9, 0, 6, 5, 0, 7, 0, 2, 5, 4, 4, 4, 0, 1, 7, 3, 8, 4, 0, 0, 4, 4, 0, 8, 9, 8, 5, 0, 2, 2, 6, 3, 1, 4, 8, 6, 1, 7, 6, 0, 2, 0, 3, 2, 2, 2, 7, 4, 2, 2, 7, 3, 9, 7, 2, 8, 0, 1, 9, 3, 0, 6, 0, 8, 8, 8, 2, 6, 3, 3, 4, 8, 0, 1, 3, 3, 8, 3, 8, 1, 6, 4, 0, 3, 4, 0, 6, 4, 7, 2, 6, 8, 3, 5, 5, 8, 4, 3, 6, 8, 5, 4, 1, 8, 3, 3, 8, 7, 2, 2, 2, 3, 0, 3, 6, 9, 7, 7, 4, 6, 6, 5, 0, 0, 2, 2, 2, 2, 6, 8, 3, 6, 6, 4, 8, 4, 9, 3, 1, 7, 7, 1, 1, 0, 4, 3, 5, 8, 9, 4, 5, 2, 7, 4, 4, 0, 9, 5, 7, 0]\n"
     ]
    }
   ],
   "source": [
    "predict_res = model.predict(Y_test[0:], verbose=1)\n",
    "# print(len(predict_res), predict_res)\n",
    "\n",
    "final_res = []\n",
    "for pre_res in predict_res:\n",
    "    final_res.append(np.argmax(pre_res))\n",
    "\n",
    "    \n",
    "# 'Japan_Travel': 0, 'KR_ENTERTAIN' 娱乐: 1, 'Makeup' 化妆 : 2, 'Tech_Job':  3, 'WomenTalk': 4, \n",
    "# 'babymother': 5, 'e-shopping': 6, 'graduate': 7, 'joke': 8, 'movie': 9}\n",
    "# 7, 9, 4, 5,    1/2, 1/8, 3/7, 1, 7, 0\n",
    "# 0, 9, 6, x(生活), 9,  9, 9, 7/3, 7, 1\n",
    "print(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "print(len(final_res))\n",
    "result_txt = \"result001\" + \".txt\"\n",
    "ids = 0\n",
    "with open(result_txt, 'w') as out:\n",
    "    out.write(\"id,category\" + '\\n')\n",
    "    for value in final_res:\n",
    "        out.write(str(ids) + \",\" + str(value) + '\\n')\n",
    "        ids += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
