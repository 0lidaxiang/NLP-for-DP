{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Homework 3\n",
    "@author A10515001 李大祥  2018-04-26\n",
    "\n",
    "# 作業3-1和處理資料\n",
    "第一個作業基本就正常按照投影片做，用 Word2Vec 這個函數生成 model 。後來根據kaggle上面同學分享的經驗改進了一下：用pkl存儲所有文本。\n",
    "\n",
    "\n",
    "# 作業3-2\n",
    "作業 3-2，一開始思路比較混亂，甚至沒有考慮到要生成文章對應類別list，也就是 label_list。開始我是對每篇文章的每個切割後字詞的 word_embedding 加總 **取平均後** 去訓練，但是這樣預測出來結果都是一個分類，明顯錯了。後來請教助教的時候發現應該是壓縮取平均的過程中 **把每篇文章的差異性消除了**，所以才會導致分成一個類別。\n",
    "\n",
    "正確的概念上的理解應該是把每一篇文章作爲一筆輸入資料 ，把一個 10 唯度的類別向量作爲一個對應的 label 。這樣訓練集有 9000 筆資料，測試結果同理有 1000 筆資料。而每筆資料中的文章，應該是這篇文章裏每個切割後的字詞的 word embedding 向量。\n",
    "\n",
    "## 在實作過程中容易走彎路和容易混淆的地方\n",
    "1. embedding_layer 必須作爲第一層輸入，它的內容是字典裏所有的 token 對應的 vector 。這個字典裏所有的 token 一般來說是不重復的，是我們能給出的訓練集裏出現過的字詞。本次作業中，我設定了每個字詞的 word_embedding 是 250 維的 vector ，所以這個 embedding_layer 的shape 應該是 250 X 232236 。\n",
    "2. 喂給神經網絡模型的 fit 函數的應該是文章序列以及對應的label，維度都是 9000 X 1 和 9000 X 10 。文章序列裏每篇文章的格式應該是數字，準確的說是每個字詞在上述字典裏的 ID 值。**在訓練的時候，keras 會自動在第一層 embedding_layer 中把這個值轉換成 word_embedding 的vector 格式** ，所以不用我們自己轉換成 word_embedding 後再輸入給 fit 函數。\n",
    "3. 由於 keras 和電腦本身更加適應統一長度的輸入，所以實作中需要把長度不一的文章填補一些 data ，以形成統一長度的格式，本次作業中統一填補成 3971 長度。因爲最長的文章就是只有 3971 個字詞。\n",
    "\n",
    "## 神經網絡模型\n",
    "第一層是 embedding_layer ，輸入的是 250 X 232236 的字典，輸出是 3971, 250 格式的一篇文章的 word_embedding 格式。\n",
    "RNN 層設定爲壓縮五倍。Dense 層只是中規中矩，輸出 10 維度的類別 vector。因爲是多類別分類，所以使用 softmax 的loss function。\n",
    "\n",
    "## 訓練部分\n",
    "### 第一次訓練\n",
    "    \n",
    "訓練過程中，第一次訓練只訓練一次，結果發現分類都是一個類別，還以爲自己寫錯了，後來嘗試過10次以後發現，其實是訓練太少的原因。\n",
    "\n",
    "正式訓練時，增加 epochs 到 50 次，修改 batch_size 到 100 次，做一次正式訓練，kaggle 結果顯示 0.12 左右。大概花費時間 3 個小時左右。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split=0.1, batch_size=100, epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再後來增加訓練次數，epochs 增加繼續訓練到 150 次，並增大 batch_size 的值到 150， kaggle 結果有明顯提升到 0.13 。這證明增加訓練次數是有效果的。batch_size 大一些，會增加梯度下降的準確性，提升模型預測的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split=0.1, batch_size=150, epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs 增加繼續訓練到 250 次，kaggle 結果有明顯提升到 0.14 。這證明增加訓練次數是有效果的,但是效果很有限。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split=0.1, batch_size=150, epochs = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二次訓練\n",
    "考慮到在上面一次訓練中增加訓練次數的效果是很有限的，所以想到一些方法來提升效果。我本來想清洗一下無用的英文字串資料數據, 並多訓練 word_embedding 的 model 結果，看看這樣效果會不會變好。但是後來請教了同學，才發現其實重点是我的神經網絡模型和资料处理有問題，于是我先处理这些重要问题。\n",
    "\n",
    "**主要問題在 3 個方面：**\n",
    "1. 處理輸入時忽略了不在字典中的字詞，加之我把所有文章補成了 3971 最長長度（實際上不應該這樣，因爲一多半的文章長度連300字詞都沒有），所以這樣子結果都不會很好。所以我修改所有文章長度統一爲 250 ，然後把不在字典中的字詞設爲 0 vector 。\n",
    "2. 訓練資料應該打亂重排序，減少規律排序對訓練的不良影響。現實中的 test 資料一般不會幫你實現排好序、劃分類群。\n",
    "3. 字典构建缺少了测试资料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 解決第一個問題，統一每個文章長度爲 250\n",
    "## 解決第二個問題，讀取資料時，打亂原有排序\n",
    "train_df_sample = pd.read_pickle('train.pkl').sample(frac=1, random_state=123)\n",
    "## 解决第三个问题： 字典构建缺少了测试资料集，而且要用同一个 tokenizer 去构建字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "並重新調整 batch_size、max_doc_word_length、epochs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle: 0.58199\n",
    "max_doc_word_length = 250\n",
    "validation_split=0.1, batch_size=3000, epochs = 100\n",
    "\n",
    "# kaggle: 0.61\n",
    "max_doc_word_length = 250\n",
    "validation_split=0.1, batch_size=1620, epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思来想去，觉得之前训练的 WordEmbedding 可能太差了，所以又去删掉多余 14 长度的无用英文单词，并反复试了几个参数，把单词之间相似读提高了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, size=250, iter= 8, workers=3, min_count=5, negative=3, max_vocab_size=None, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再之后我去检查了字词出现的次数，发现大概15万的字词都是少于3次的，我尝试了删去这些字词，把总参数从5000多万降低到：10,474,876。但发现结果并没有很好，而且很难提升，所以也就没有去kaggle测试效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split=0.1, batch_size=405, epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以后来还是回归原先的模型，只是删除了多余 14 长度的无用英文单词和加入测试资料集后重构了字典，其余保留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle: 0.79700\n",
    "validation_split=0.1, batch_size= 1620, epochs = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "總結來說，收获如下：\n",
    "1. 因爲訓練資料集爲 9000 ，而且又是使用 RNN 訓練分類 ，所以 batch_size 不應該過小（比如 150 ），否則會出現訓練出來後幾乎無法分類、分類準確率很低。\n",
    "2. max_doc_word_length 設置爲 250 ，是因爲算過所有訓練文章的平均長度，差不多在 210 左右，但是也有幾百篇文章是超過 800 多個字詞甚至三千多字詞的，所以設置爲 200 ～ 300 左右還算合理。\n",
    "3. epochs 次數大概在 200 ～ 400 左右會有一個比較好的 model 結果，只訓練一兩次用來測試幾乎無法分類。\n",
    "4. 出现问题最多的在于如何处理资料，比如去掉杂讯、乱序重排、构建字典要足够完整并且不能构建两个否则字词id会乱掉。keras 自带的 library 很好用，但是也很容易用错，要多看文档。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
