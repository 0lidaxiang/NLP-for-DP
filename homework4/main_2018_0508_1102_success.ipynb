{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout, Flatten, Conv2D, Conv1D, MaxPooling1D, MaxPooling2D,Activation\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from datetime import datetime\n",
    "from gensim.models import word2vec\n",
    "from collections import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_index(corpus):\n",
    "    new_corpus = []\n",
    "    for doc in corpus:\n",
    "        new_doc = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                new_doc.append(word2idx[word])\n",
    "            except:\n",
    "                new_doc.append(0)\n",
    "#         new_doc_arr = np.array(new_doc).reshape(1, max_doc_word_length)\n",
    "        new_doc_arr = np.array(new_doc)\n",
    "        new_corpus.append( new_doc_arr)\n",
    "    return np.array(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "# get texts data\n",
    "category2idx = {'AllTogether': 0, 'Baseball': 1, 'Boy-Girl': 2, 'C_chat':  3, 'CVS': 4,\n",
    "                  'GameSale': 5, 'GetMarry': 6, 'Lifeismoney': 7, 'LoL': 8, 'MH': 9, 'MLB': 10, 'Mobilecomm': 11, \n",
    "                'movie': 12,'MuscleBeach':  13, 'NBA': 14,  'SENIORHIGH': 15, 'Stock': 16, \n",
    "                'Tennis': 17, 'Tos': 18, 'WomenTalk': 19}\n",
    "\n",
    "train_df_sample = pd.read_pickle('train.pkl').sample(frac=1, random_state=123)\n",
    "train_texts = train_df_sample.values\n",
    "label_list = train_df_sample.label\n",
    "\n",
    "test_pickle_df = pd.read_pickle('test.pkl')\n",
    "test_texts = test_pickle_df[\"text\"].values\n",
    "\n",
    "train_texts_list = []\n",
    "for text in train_texts:\n",
    "    train_texts_list.append(text[0])\n",
    "    \n",
    "texts_list = []\n",
    "for text in train_texts_list:\n",
    "    texts_list.append(text)\n",
    "    \n",
    "for text in test_texts:\n",
    "    texts_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word embedding vector\n",
    "answer = word2vec.Word2Vec.load(\"word2vec_20180430.model\")\n",
    "word_vectors = answer.wv\n",
    "wvv = word_vectors.vocab\n",
    "wvv_keys = wvv.keys()\n",
    "wvv_keys_list = list(wvv_keys)\n",
    "\n",
    "vocab_num = len(wvv.items()) + 1\n",
    "\n",
    "vocab_list = [(word, word_vectors[word]) for word, _ in wvv.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del word_vectors, wvv, train_texts_list, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_len = 256\n",
    "embedding_matrix = np.zeros((vocab_num , word_vec_len))\n",
    "word2idx = {}\n",
    "\n",
    "for i, vocab in enumerate(vocab_list):\n",
    "    word, vec = vocab\n",
    "    embedding_matrix[i + 1] = vec\n",
    "    word2idx[word] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 256)          22472704  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 256)          196864    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102402    \n",
      "=================================================================\n",
      "Total params: 22,771,970\n",
      "Trainable params: 299,266\n",
      "Non-trainable params: 22,472,704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding( input_dim= embedding_matrix.shape[0],output_dim= 256, weights=[embedding_matrix], \n",
    "                            input_length = 200,trainable=False)\n",
    "# model = getModel(embedding_layer, image_width, image_height, input_channel)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer )\n",
    "model.add(Conv1D(256, 3,padding = 'same', ))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.compile(optimizer='adam',loss='mae')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_doc_word_length = 200\n",
    "\n",
    "x_trains_texts = train_df_sample.text.append(test_pickle_df.text)\n",
    "X_train_texts = text_to_index(x_trains_texts)\n",
    "X_train = pad_sequences(X_train_texts, maxlen= max_doc_word_length)\n",
    "\n",
    "Y_label_list = np.zeros((36000, 2))\n",
    "for ids in range(0, 36000):\n",
    "    Y_label_list[ids][0] = label_list[ids][0]\n",
    "    Y_label_list[ids][1] = label_list[ids][1]\n",
    "print(Y_label_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "36000/36000 [==============================] - 109s 3ms/step - loss: 11.8380 - acc: 0.9501\n",
      "Epoch 2/35\n",
      "36000/36000 [==============================] - 109s 3ms/step - loss: 11.5550 - acc: 0.9705\n",
      "Epoch 3/35\n",
      "36000/36000 [==============================] - 111s 3ms/step - loss: 11.3904 - acc: 0.9705\n",
      "Epoch 4/35\n",
      "36000/36000 [==============================] - 111s 3ms/step - loss: 11.2550 - acc: 0.9705\n",
      "Epoch 5/35\n",
      "36000/36000 [==============================] - 111s 3ms/step - loss: 11.1396 - acc: 0.9705\n",
      "Epoch 6/35\n",
      "36000/36000 [==============================] - 115s 3ms/step - loss: 11.0301 - acc: 0.9705\n",
      "Epoch 7/35\n",
      "36000/36000 [==============================] - 113s 3ms/step - loss: 10.9349 - acc: 0.9705\n",
      "Epoch 8/35\n",
      "36000/36000 [==============================] - 113s 3ms/step - loss: 10.8522 - acc: 0.9705\n",
      "Epoch 9/35\n",
      "36000/36000 [==============================] - 113s 3ms/step - loss: 10.7565 - acc: 0.9705\n",
      "Epoch 10/35\n",
      "36000/36000 [==============================] - 116s 3ms/step - loss: 10.6718 - acc: 0.9705\n",
      "Epoch 11/35\n",
      "36000/36000 [==============================] - 118s 3ms/step - loss: 10.5998 - acc: 0.9705\n",
      "Epoch 12/35\n",
      "36000/36000 [==============================] - 112s 3ms/step - loss: 10.5281 - acc: 0.9705\n",
      "Epoch 13/35\n",
      "36000/36000 [==============================] - 110s 3ms/step - loss: 10.4600 - acc: 0.9705\n",
      "Epoch 14/35\n",
      "36000/36000 [==============================] - 111s 3ms/step - loss: 10.4004 - acc: 0.9705\n",
      "Epoch 15/35\n",
      "36000/36000 [==============================] - 110s 3ms/step - loss: 10.3285 - acc: 0.9705\n",
      "Epoch 16/35\n",
      "36000/36000 [==============================] - 112s 3ms/step - loss: 10.2570 - acc: 0.9705\n",
      "Epoch 17/35\n",
      "36000/36000 [==============================] - 110s 3ms/step - loss: 10.1966 - acc: 0.9705\n",
      "Epoch 18/35\n",
      "36000/36000 [==============================] - 110s 3ms/step - loss: 10.1437 - acc: 0.9705\n",
      "Epoch 19/35\n",
      "36000/36000 [==============================] - 113s 3ms/step - loss: 10.0927 - acc: 0.9705\n",
      "Epoch 20/35\n",
      "36000/36000 [==============================] - 114s 3ms/step - loss: 10.0410 - acc: 0.9705\n",
      "Epoch 21/35\n",
      "36000/36000 [==============================] - 116s 3ms/step - loss: 9.9913 - acc: 0.9705\n",
      "Epoch 22/35\n",
      "36000/36000 [==============================] - 115s 3ms/step - loss: 9.9345 - acc: 0.9705\n",
      "Epoch 23/35\n",
      "36000/36000 [==============================] - 118s 3ms/step - loss: 9.8931 - acc: 0.9704\n",
      "Epoch 24/35\n",
      "36000/36000 [==============================] - 121s 3ms/step - loss: 9.8417 - acc: 0.9704\n",
      "Epoch 25/35\n",
      "18000/36000 [==============>...............] - ETA: 1:00 - loss: 9.7200 - acc: 0.9698"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(x = X_train[0:36000], \n",
    "                    y = Y_label_list, \n",
    "                    batch_size= 1800,\n",
    "                    epochs = 35, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np_loss_history = np.array(history.history['loss'])\n",
    "np.savetxt(\"loss_history.txt\", np_loss_history, delimiter=\",\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "x = np.linspace(0, 1, np_loss_history.shape[0])\n",
    "loss_history = np.loadtxt(\"loss_history.txt\")\n",
    "plt.plot(x, loss_history, '-g');  # dotted red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss_accuracy = model.evaluate(X_train[0:100], Y_label_list[0:100], verbose=1)\n",
    "print(type(loss_accuracy), loss_accuracy)\n",
    "\n",
    "test_sequences1 = X_train[36000:40000]\n",
    "\n",
    "predict_res = model.predict(test_sequences1, batch_size= 3600, verbose=0)\n",
    "\n",
    "final_res = []\n",
    "for pre_res in predict_res:\n",
    "    final_res.append(pre_res)\n",
    "# print(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_txt = \"result\" + str(datetime.now()).split()[1] + \".txt\"\n",
    "print(len(final_res))\n",
    "result_txt = \"local_result001\" + \".txt\"\n",
    "ids = 0\n",
    "with open(result_txt, 'w') as out:\n",
    "    out.write(\"id,good,bad\" + '\\n')\n",
    "    for value in final_res:\n",
    "        out.write(str(ids) + \",\" + str(int (round(value[0]))) + \",\" + str(int (round(value[0]))) + '\\n')\n",
    "        ids += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
