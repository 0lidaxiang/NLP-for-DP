{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 µs, sys: 3 µs, total: 24 µs\n",
      "Wall time: 30.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = './data/Test/'\n",
    "TRAINING_PATH = './data/Training/'\n",
    "TESTING_PATH = './data/Test/'\n",
    "\n",
    "category2idx = {'AllTogether': 0, 'Baseball': 1, 'Boy-Girl': 2, 'C_chat':  3, 'CVS': 4,\n",
    "                  'GameSale': 5, 'GetMarry': 6, 'Lifeismoney': 7, 'LoL': 8, 'MH': 9, 'MLB': 10, 'Mobilecomm': 11, \n",
    "                'movie': 12,'MuscleBeach':  13, 'NBA': 14,  'SENIORHIGH': 15, 'Stock': 16, \n",
    "                'Tennis': 17, 'Tos': 18, 'WomenTalk': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie\n",
      "SENIORHIGH\n",
      "Boy-Girl\n",
      "GetMarry\n",
      "WomenTalk\n",
      "Mobilecomm\n",
      "LoL\n",
      "Lifeismoney\n",
      "MuscleBeach\n",
      "Tos\n",
      "MH\n",
      "Tennis\n",
      "CVS\n",
      "GameSale\n",
      "Baseball\n",
      "MLB\n",
      "Stock\n",
      "C_chat\n",
      "AllTogether\n",
      "NBA\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "\n",
    "for category in category2idx:\n",
    "    category_idx = category2idx[category]\n",
    "    category_path = TRAINING_PATH + category + '_cut/'\n",
    "    print(category)\n",
    "        \n",
    "    # get label data\n",
    "    # C_chatans_0_1799.txt\n",
    "    label_dict = dict()\n",
    "    label_file_path = TRAINING_PATH + category + 'ans_0_1799.txt'\n",
    "    with open(label_file_path, encoding='utf-8') as file:\n",
    "            label_words = file.read().strip().split()\n",
    "            for a_label in label_words:\n",
    "                label_split1 = a_label.split(\"推\")\n",
    "                train_file_id = label_split1[0]\n",
    "                label_split2 = label_split1[1].split(\"噓\")\n",
    "                label_dict[int(train_file_id)] = label_split2\n",
    "    # print(\"123 \" , label_dict, label_dict[0][0],)\n",
    "            \n",
    "    # get train data\n",
    "    for filename_id in range(0, 1800):\n",
    "        filename = category + '_cut' + str(filename_id) + \".txt\"\n",
    "        filepath = category_path + filename\n",
    "       \n",
    "        with open(filepath, encoding='utf-8') as file:\n",
    "            words = file.read().strip().split(' / ')\n",
    "            for word in words:\n",
    "                if len(word) > 14:\n",
    "                    words.remove(word)\n",
    "                elif word[0:-6].encode( 'UTF-8' ).isalpha():\n",
    "                    words.remove(word)\n",
    "                elif word[0:9].encode( 'UTF-8' ).isalpha():\n",
    "                    words.remove(word)\n",
    "            train_list.append([words, category_idx, label_dict[filename_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_files = [labelFileName for labelFileName in os.listdir(TRAINING_PATH) if labelFileName[-4:] != '_cut']\n",
    "label_list = []\n",
    "for label_file in label_files:\n",
    "     with open(filepath, encoding='utf-8') as file:\n",
    "        words = file.read().strip().split(' / ')\n",
    "        for word in words:\n",
    "                if len(word) > 14:\n",
    "                    words.remove(word)\n",
    "                elif word[0:-6].encode( 'UTF-8' ).isalpha():\n",
    "                    words.remove(word)\n",
    "                elif word[0:9].encode( 'UTF-8' ).isalpha():\n",
    "                    words.remove(word)\n",
    "        label_list.append([words, category_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(label_files), str(label_files[0]))\n",
    "# print(len(label_list), train_list[0],str(label_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_r = 0\n",
    "# for arti in train_list:\n",
    "#     sum_r += len(arti[0])\n",
    "# print(sum_r , 1892779/9000)\n",
    "# # 7074422 210.30877777777778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (36000, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(train_list, columns=[\"text\", \"category\", \"label\"])\n",
    "print(\"Shape:\", train_df.shape)\n",
    "train_df.sample(5)\n",
    "\n",
    "train_df.to_pickle('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = []\n",
    "\n",
    "for idx in range(4000):\n",
    "    filepath = TESTING_PATH + str(idx) + '.txt'\n",
    "    \n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        words = file.read().strip().split(' / ')\n",
    "        for word in words:\n",
    "                if len(word) > 14:\n",
    "                    words.remove(word)\n",
    "                elif word[0:-6].encode( 'UTF-8' ).isalpha():\n",
    "                    words.remove(word)\n",
    "                elif word[0:9].encode( 'UTF-8' ).isalpha():\n",
    "                    words.remove(word)\n",
    "        test_list.append([idx, words])\n",
    "        \n",
    "test_df = pd.DataFrame(test_list, columns=[\"id\", \"text\"])\n",
    "print(\"Shape:\", test_df.shape)\n",
    "test_df.sample(5)\n",
    "\n",
    "test_df.to_pickle('test.pkl')\n",
    "pickle_df = pd.read_pickle('test.pkl')\n",
    "test_df.equals(pickle_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pickle_df = pd.read_pickle('train.pkl')\n",
    "test_texts = test_pickle_df[\"text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( test_texts[0:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
